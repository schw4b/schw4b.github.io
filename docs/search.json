[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon Schwab",
    "section": "",
    "text": "“I’m addicted to coffee and data.”"
  },
  {
    "objectID": "index.html#qualifications",
    "href": "index.html#qualifications",
    "title": "Simon Schwab",
    "section": "Qualifications",
    "text": "Qualifications\nGradStat—Graduate Statistician\nRoyal Statistical Society, 2023\nCAS—Advanced Statistical Data Science\nUniversity of Bern, 2022\nPhD—Health Sciences\nUniversity of Bern, 2013\nMSc—Quantitative Psychology & Computer Science\nUniversity of Bern, 2008"
  },
  {
    "objectID": "index.html#current-positions",
    "href": "index.html#current-positions",
    "title": "Simon Schwab",
    "section": "Current positions",
    "text": "Current positions\nLecturer in Biostatistics (5%)\nDepartment of Biostatistics, University of Zurich\n09/2023—Present\nSenior Statistician (80%)\nSwisstransplant, Bern\n11/2021—Present"
  },
  {
    "objectID": "index.html#past-positions",
    "href": "index.html#past-positions",
    "title": "Simon Schwab",
    "section": "Past positions",
    "text": "Past positions\nPostdoctoral Researcher\nCenter for Reproducible Science, University of Zurich\n08/2018—09/2021\nResearch Fellow\nBig Data Institute, University of Oxford\n07/2017—06/2018\nVisiting Academic\nDepartment of Statistics, University of Warwick\n01/2016—06/2017\nResearch Associate\nUniversity Hospital of Psychiatry, Bern\n10/2008—12/2015"
  },
  {
    "objectID": "posts/1901/index.html",
    "href": "posts/1901/index.html",
    "title": "1901",
    "section": "",
    "text": "It was 2004, I was a student, and at that time, I was really in love with Linux. One day, I saw an IBM commercial, “The Kid”. There were so many cool messages in this commercial; it kind of blew my mind. In the end, Muhammad Ali appeared and delivered the following line to the kid.\nSince IBM loved Linux, too, I wanted to work for IBM. I applied for an internship and was eventually accepted. As I already started University, I had to take a break from my studies. Before I realized it, I was an intern at the data center of IBM Switzerland.\nAfter six months, I could take a lot of stuff home. I worked on a large project for the first time. I reported to a manager and had deliverables, and I taught myself the command language Bash from reading Newham & Rosenblatt [1]. The things I learned back then still largely influence my daily interactions with computers today, which is by using a Unix type of shell program.\nDuring my internship, I bought an IBM ThinkCentre S50. On this machine, I learned programming, completed my software engineering classes, and wrote my first computer program in Java."
  },
  {
    "objectID": "posts/1901/index.html#computer-age",
    "href": "posts/1901/index.html#computer-age",
    "title": "1901",
    "section": "Computer age",
    "text": "Computer age\nIt was 2024, a rainy day in October. I went to the basement to get rid of old stuff. I found my old IBM ThinkCentre S50 from 2004. It was still there.\nWhat could you do with a 20-year-old computer in 2024? Not much, I guessed. The plan was to install a lightweight Linux distribution and the latest R, RStudio, and Quarto.\nI also found a M52 in my basement, which was a slightly newer model from 2005. The two models, the S50 and the M52, were very similar; in fact, both had a Pentium 4 3.0 GHz processor and 1GB of memory. However, the S50 was the Intel Northwood architecture (32-bit), while the M52 was the Prescott architecture (64-bit). I didn’t want to install a 32-bit (i386) operating system. Major Linux distributions have dropped i386 support, for example, Ubuntu in 2019.\nThus, I decided to go for the M52; however, the S50 had a floppy drive. It would be a real shame to make a compromise here. Fortunately, the M52 had a tray for the floppy drive and a connector on the main board. I just had to install the floppy drive and replace the M52 front bezel with the one from the S50 because of the opening for the floppy drive.\nThe result was an M52 with the look of an S50, brilliant; see a photo in Figure 1.\n\n\n\nFigure 1: IBM ThinkCentre M52 with S50 front bezel and floppy drive.\n\n\nI spent a little money to upgrade the machine from 1GB memory to 4GB memory (purchased online; CHF 7.50). I also replaced the hard disk, which was an HDD, with a modern SSD that I had as a spare. Then I installed Lubuntu 24.04 LTS, a lightweight Linux based on Ubuntu, R 4.4.1, and the latest version of RStudio and Quarto.\nI collected some system information using the system() function which called neofetch from within this Quarto Blog post, see Table 1. Exactly, I wrote this blog post using the old IBM computer.\n\n\nCode\ninfo = system(\"neofetch --stdout\", intern = TRUE)\nidx = c(4, 15, 17, 16, 3, 5) # select items to show\ninfo = strsplit(info, \": \")\ntab = do.call(\"rbind\", info[idx])\ntab = as.data.frame(tab)\ncolnames(tab) = c(\"Type\", \"System Information\")\ntab\n\n\n\n\n\nTable 1: System information that was directly collected while this Quarto blog post was rendered on my almost 20-year-old IBM machine.\n\n\n\n\n\nType\nSystem Information\n\n\n\n\nHost\n8215Y6G ThinkCentre M52\n\n\nCPU\nIntel Pentium 4 3.00GHz (2) @ 1.500GHz\n\n\nMemory\n1540MiB / 3222MiB\n\n\nGPU\nNVIDIA GeForce GT 610\n\n\nOS\nLubuntu 24.04.1 LTS x86_64\n\n\nKernel\n6.8.0-48-generic\n\n\n\n\n\n\nI wondered why I only had 3GB of memory available, even though I installed 4GB. Looking at the technical specifications of the M53, I found the following line:\n\nIf 4GB of physical memory is installed approximately 3GB will be useable by the operating system. This is due to limitations from the address mapping of PCI busses and other system devices.\n\nWell, that was a shame, but okay, at least there was nothing wrong on my side."
  },
  {
    "objectID": "posts/1901/index.html#tweaks-and-software-installation",
    "href": "posts/1901/index.html#tweaks-and-software-installation",
    "title": "1901",
    "section": "Tweaks and software installation",
    "text": "Tweaks and software installation\nI made two tweaks to the Lubunu installation. First, I wanted to see the boot messages during startup instead of the Lubuntu logo.\nsudo vim /etc/default/grub\n# GRUB_CMDLINE_LINUX_DEFAULT='quiet splash'\nGRUB_CMDLINE_LINUX_DEFAULT=\"\"\nsudo update-grub\nSecond, I wanted a profile image for my account to appear on the login screen; this can be done by creating a PNG image file in the home directory named .face.icon. My profile image had a resolution of 400x400, but it was a JPEG file; therefore, I had to convert it first. Also, the login manager SDDM needed to read this file, so I had to set the correct permissions:\nconvert profile.jpg profile.png\nmv profile.png .face.icon\nsetfacl -m u:sddm:r ~/.face.icon\nI installed R directly from the R Project repository for the latest version. For that, the following entry in the sources.list was needed:\nsudo vim /etc/apt/sources.list # edit file and add line below\ndeb https://cloud.r-project.org/bin/linux/ubuntu noble-cran40/\nIt was important to add a security key to verify the R Project repository; see detailed instructions.\nThen, I installed R as follows:\nsudo apt-get update\nsudo apt-get install r-base\nI downloaded the R Studio from the official website and installed it; I selected the newest available package, which was for Ubuntu 22/Debian 12. My first attempt to install RStudio failed because two other packages needed to be installed first:\nsudo apt-get install libssl-dev\nsudo apt-get install libclang-dev\nsudo dpkg -i rstudio-2024.09.0-375-amd64.deb\nFinally, I installed Quarto.\nsudo dpkg -i quarto-1.5.57-linux-amd64.deb"
  },
  {
    "objectID": "posts/1901/index.html#a-phoenix",
    "href": "posts/1901/index.html#a-phoenix",
    "title": "1901",
    "section": "A phoenix?",
    "text": "A phoenix?\nI logged into my new system and started RStudio (Figure 2). The computer took 1 minute and 2 seconds to boot, and RStudio took about 22 seconds to load. Since I rendered the documents from the command line using quarto preview, I could have just used a lightweight editor.\n\n\n\nFigure 2: Screenshot of the Lubunut 24.04 desktop running RStudio on an old IBM Pentium 4 machine.\n\n\nI wanted to know if I could do a statistical analysis with a modern operating system and state-of-the-art statistics software from 2024 on this 20-year-old machine, but it was possible. It was like the past and present finally coming together to create something new.\n\nPast and present, they don’t matter. Now the future’s sorted out. —1901 (song by Phoenix)\n\nThe machine obtained a new life, and that was because Linux is awesome. Did I already mention that I wrote this blog post, including all the computations and statistical analyses, on this old IBM computer described here?\nThe title of this post is a song by Phoneix; the preview image of this post is a photo by Denny Müller on Unsplash."
  },
  {
    "objectID": "posts/1901/index.html#just-one-more-thing",
    "href": "posts/1901/index.html#just-one-more-thing",
    "title": "1901",
    "section": "Just one more thing",
    "text": "Just one more thing\nI performed a statistical analysis of the Titanic data with the 20-year-old IBM computer. I used the TitanicSurvival data set from the carData package. I also used rms and my package swt because I wrote my own tidy functions to display regression results.\n\n\nCode\nlibrary(carData)\nlibrary(rms)\n# installed from https://github.com/Swisstransplant/swt\nlibrary(swt)\n\n\n\n\nCode\ndata = TitanicSurvival\ndata$age = round(data$age, digits = 1)\n\n\nAccording to Wikipedia, 2240 people were on the Titanic, and 1517 (68%) persons died. However, the data only contained N=1309 persons with 500 (38%) deaths. Thus, there was a lot of missing data: I only had 58% of the persons and 33% of the deaths.\nAnyway, the results below may have a high risk of bias. Maybe there was a better data set I could have used? The data are shown in Table 2.\n\n\nCode\nset.seed(2024)\nidx = sample(nrow(data), 6)\n\ndata[idx,]\n\n\n\n\nTable 2: Six randomly selected passengers with recorded survival status, sex, age, and passenger class.\n\n\n\n\n\n\n\n\n\n\nsurvived\nsex\nage\npassengerClass\n\n\n\n\nRichards, Master. George Sibley\nyes\nmale\n0.8\n2nd\n\n\nCacic, Mr. Luka\nno\nmale\n38.0\n3rd\n\n\nVendel, Mr. Olof Edvin\nno\nmale\n20.0\n3rd\n\n\nKaraic, Mr. Milan\nno\nmale\n30.0\n3rd\n\n\nAndersen, Mr. Albert Karvin\nno\nmale\n32.0\n3rd\n\n\nEustis, Miss. Elizabeth Mussey\nyes\nfemale\n54.0\n1st\n\n\n\n\n\n\nI wanted to predict survival based on passenger characteristics. I fitted a logistic regression model. The outcome was survived (“yes”), and the predictors were sex, age, and passenger class. The results are shown in Table 3.\n\n\nCode\ndd = datadist(data)\noptions(datadist='dd')  \n\nfit = lrm(survived ~ sex + rcs(age, 3) + passengerClass, data = data)\nswt::tidy_rmsfit(fit)\n\n\n\n\nTable 3: Results from a logistic regression model.\n\n\n\n\n\n\n\n\n\n\n\nInterquartile difference\nOdds ratio (95%-CI)\nChi-Square\nd.f.\np-value\n\n\n\n\nage\n18.00 (21.00–39.00)\n0.53 (from 0.42 to 0.66)\n30.8\n2\n&lt; 0.001 ***\n\n\nage nonlinear\n–\n–\n1.1\n1\n0.29\n\n\npassengerClass\n–\n–\n101\n2\n&lt; 0.001 ***\n\n\npassengerClass 1st\n–\n9.65 (from 6.19 to 15.04)\n–\n–\n–\n\n\npassengerClass 2nd\n–\n2.77 (from 1.88 to 4.09)\n–\n–\n–\n\n\nsex\n–\n–\n225\n1\n&lt; 0.001 ***\n\n\nsex female\n–\n12.13 (from 8.76 to 16.80)\n–\n–\n–\n\n\nTOTAL\n–\n–\n277\n5\n&lt; 0.001 ***\n\n\n\n\n\n\nThe most relevant factors were sex and passenger class. The odds for a female to survive were about 12.1 times higher compared to a male. The odds for a 1st class passenger to survive were about 9.7 times higher, and for a 2nd class passenger, about 2.8 times higher compared to a 3rd class passenger. Age also had an effect; the older the person, the less likely they survived. For example, the odds for a 39-year-old person to survive were only about 0.5 times compared to a younger 21-year-old.\n\nSo what?\nIf you were male, 3rd class, or old, you were more likely to die in the Titanic incident. However, this was only a quick showcase rather than an in-depth analysis of the Titanic data, and the large amount of missing data was not ideal. But I wanted to say something else.\nI wanted to say, don’t throw away old computers. They can still be fun."
  },
  {
    "objectID": "posts/confusion/index.html",
    "href": "posts/confusion/index.html",
    "title": "Land of confusion",
    "section": "",
    "text": "Have you ever been confused about data and evidence? Frankly, I find myself confused in data many times. The real goal isn’t to eliminate confusion entirely but to reduce it and gain some sense of certainty.\nOne such thing that can be truly confusing in medical research is confounding.\nBut what is confounding? Confounding bias occurs when a third variable is associated with a risk factor (or treatment) and also influences the health outcome, leading to a wrong result. This can either create a false relationship or hide a real one.\nI make up an example. Say, doctors have higher cancer rates. The confounding factor may be smoking if doctors tend to smoke more, and smoking causes cancer. So, no, being a doctor per se does not cause cancer at all.\nAfter a literature search, I found two excellent examples from published medical studies where confounding occurred.\nIn the 1990s, observational studies suggested that hormone replacement therapy (HRT) reduced the risk of cardiovascular disease (CVD) in women [1]. However, this association was most likely false. Subsequent randomized controlled trials and a meta-analysis from 2002 found no evidence that HRT reduced the risk of CVD [2]. The apparent benefits were likely influenced by confounders such as socioeconomic status and education. Lower socioeconomic status is a strong risk factor for CVD, coronary artery disease, and many other adverse health outcomes. Additionally, women using HRT tend to have higher socioeconomic status, which may partially explain their better outcomes in earlier studies.\nA 2016 observational study compared tracheal intubation to bag-valve-mask ventilation during pediatric cardiac arrest [3]. The study found that survival was lower in those children intubated compared with those not intubated. The authors concluded that the study does not support early tracheal intubation for pediatric in-hospital cardiac arrest. However, the study suffered from confounding by indication [4]. It was likely that children with a more severe condition and worse overall prognosis had a greater probability of being intubated. Consequently, the treatment group showed a higher risk of death.\nThe two examples show that confounding can confuse doctors in making the right choice based on solid evidence. Needless to say, this can potentially harm patients."
  },
  {
    "objectID": "posts/confusion/index.html#how-can-we-reduce-confounding",
    "href": "posts/confusion/index.html#how-can-we-reduce-confounding",
    "title": "Land of confusion",
    "section": "How can we reduce confounding?",
    "text": "How can we reduce confounding?\nThere are several ways to address confounding in medical research. One is study design, including randomized controlled trials (RCTs), which are highly effective and provide strong evidence.\nSay, we are interested in the therapeutic effect of hypothermic machine perfusion (HMP) in kidney transplantation. We could perform an RCT and randomize the kidneys into a control group (standard cold storage) and an intervention group (HMP).\nHowever, an RCT is not always possible. For example, if we want to assess the risk of donation after brain death (DBD) versus donation after circulatory death (DCD) kidney transplants, it is not possible to randomly assign kidneys to either group. Similarly, just as we cannot randomly assign individuals to be smokers or non-smokers, certain real-world conditions make randomization impossible.\nThere are also other approaches to tackle confounding, including matching and propensity score methods, which pair participants with similar clinical characteristics to reduce confounding [5,6]. It is important to carefully select the matching variables, choose an appropriate matching method, and perform a diagnostic assessment of the matching process. Additionally, stratified analysis helps control for confounding by analyzing data within specific subgroups.\nIn the remainder of this post, I will focus on another key approach: multivariable regression modeling. This method allows researchers to account for multiple confounders simultaneously by including them as covariates in a regression model, helping to isolate the true relationship between the exposure and outcome of interest."
  },
  {
    "objectID": "posts/confusion/index.html#everything-will-mislead-you-except-a-model",
    "href": "posts/confusion/index.html#everything-will-mislead-you-except-a-model",
    "title": "Land of confusion",
    "section": "Everything will mislead you except a model",
    "text": "Everything will mislead you except a model\n\nA model is a lie that helps you see the truth. —Howard E. Skipper\n\nI’m a big fan of model-based approaches [7,8]. Models provide the most comprehensive description of data. Unlike traditional statistical hypothesis tests, which focus primarily on detecting differences, models emphasize estimation, making results more meaningful for patients and clinicians.\nOne major advantage of models is their ability to adjust for multiple factors simultaneously, providing more accurate effect estimates. While researchers often focus on multiplicity adjustments for p-values, they rarely apply similar adjustments to effect estimates. Models, however, allow for these adjustments because the effect of interest is fully conditional on covariate values. In other words, model-based methods are much more powerful and reliable than analyzing variables in isolation.\nFor instance, consider a scenario in kidney transplantation, where we want to assess the impact of the deceased donor’s age increasing from 50 to 65 while holding other risk factors constant. A simple subgroup comparison—such as analyzing younger versus older donors doesn’t provide a reasonable estimate of this specific effect. A model, however, allows us to quantify the risk associated with this change while controlling for other variables as well.\nModels also generalize statistical tests. For example, a t-test and ANOVA can be performed using multiple linear regression. Similarly, the Wilcoxon test and Kruskal-Wallis test correspond to a proportional odds model, while the log-rank test is a special case of the Cox proportional hazards model.\nUltimately, a model-based approach is the most accurate description of your data. However, it cannot definitively establish causality without careful attention to the research question, causal estimand, study design, and underlying assumptions [9].\nIn the end, you have the choice: get confused by analyzing variables individually or let a statistical model confuse you all at once.\nThe title of this post is a song by Genesis; the preview image of this post is a photo by Katie Moum on Unsplash."
  },
  {
    "objectID": "posts/confusion/index.html#just-one-more-thing",
    "href": "posts/confusion/index.html#just-one-more-thing",
    "title": "Land of confusion",
    "section": "Just one more thing",
    "text": "Just one more thing\nI performed a simulation study with an example from the heart transplant waiting list. Again, this is not real data; I made this up. All data were generated from the following assumptions.\n\nI generated data for 2,000 patients\n25% were female (500 patients) and 75% were male (1500 patients)\nFemales had an average weight of 65 kg with a standard deviation of 10 kg\nMales had an average weight of 80 kg with a standard deviation of 10 kg\n\n\n\nCode\nset.seed(2025)\n\n# Install the swt package\n# remotes::install_github(\"Swisstransplant/swt\")\nlibrary(swt)\nlibrary(rms)\nlibrary(ggplot2)\n\nN = 2000\nn1 = 0.25*N # female \nn2 = 0.75*N # male\n\nheart = data.frame(weight = rep(NA, N),\n                   sex = c(rep(\"female\", n1), rep(\"male\", n2))\n)\n\nheart$weight[heart$sex == \"female\"] = rnorm(n1, mean = 65, sd = 10)\nheart$weight[heart$sex == \"male\"] = rnorm(n2, mean = 80, sd = 10)\n\n\nNext, I generated the probabilities for a heart transplant as an inverse function of body weight. As body weight increases, the likelihood of receiving a matching heart transplant decreases. Research supports this inverse relationship between body weight and transplant probability: larger individuals require greater cardiac output, and a smaller donor heart may struggle to meet these demands, potentially leading to poor circulation and organ failure [10].\nI sampled from a binomial distribution to determine whether each patient received a transplant or not based on their individual probability. In my simulation, the probability of a transplant depended solely on body weight and not on sex. Precisely, the probability was constant at 0.80 until 65 kg; then it decreased as a quadratic function.\n\n\nCode\n# function with constant prob until 65, then declines\nprob_fun &lt;- function(x) {\n  prob = ifelse(x &lt;= 65, 0.8, 0.8 - (x - 65)^2 / 2200)\n  return(pmax(prob, 0))  \n}\n\nheart$P = sapply(heart$weight, FUN = prob_fun)\nheart$P = heart$P + rnorm(N, 0, sd = 0.1) # add some noise\nheart$P[heart$P &gt; 1] = 1 # truncate probabilities\nheart$P[heart$P &lt; 0] = 0\n\nheart$event = rbinom(N, size = 1, prob = heart$P)\nheart$event = heart$event == 1 # binary conversion\n\n\nThe simulated data for 10 randomly picked patients are shown in Table 1. Figure 1 and Table 2 display descriptives of the data.\n\n\nCode\nset.seed(1986)\nidx = sample(nrow(heart), 7)\nheart[idx,]\n\n\n\n\nTable 1: Data from seven randomly picked patients. P is the probability of a heart transplant as a function of body weight; the event is a binary variable indicating whether a patient received a heart transplant.\n\n\n\nweight\nsex\nP\nevent\n\n\n\n\n840\n72.26087\nmale\n0.8648124\nTRUE\n\n\n1434\n93.79369\nmale\n0.3070354\nFALSE\n\n\n1083\n67.71200\nmale\n0.7036489\nTRUE\n\n\n1219\n93.54775\nmale\n0.5773796\nTRUE\n\n\n406\n73.69477\nfemale\n0.7522511\nTRUE\n\n\n142\n45.84125\nfemale\n0.8791807\nTRUE\n\n\n737\n83.48071\nmale\n0.6698299\nTRUE\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 3))\n\nhist(heart$weight, ylab = \"Count\", xlab = \"Body weight (kg)\", col = \"#818C70\", main=NULL, \n     cex.lab = 1.3, cex.axis = 1.3)\nhist(heart$P, ylab = \"Count\", xlab = \"Transplant probability\", col = \"#BF505A\", main=NULL,\n     cex.lab = 1.3, cex.axis = 1.3)\n#plot(heart$weight, sapply(heart$weight, FUN = prob_fun))\nplot(heart$weight, heart$P,\n     xlab = \"Body weight (kg)\", ylab = \"Transplant probability\", col = \"#6F87A6\",\n     cex.lab = 1.3, cex.axis = 1.3)\n\n\n\n\n\nFigure 1: Distribution of body weight, transplant probabilities, and their relationship.\n\n\n\n\n\n\nCode\ntab = data.frame(N = nrow(heart),\n                 weight = swt::mean_sd(heart$weight),\n                 transplant = swt::count_perc(heart$event)\n)\n\nheart.males   = subset(heart, subset = sex == \"male\")\nheart.females = subset(heart, subset = sex == \"female\")\n\ntab.m = data.frame(N = nrow(heart.males),\n                   weight = swt::mean_sd(heart.males$weight),\n                   transplant = swt::count_perc(heart.males$event)\n)\n\ntab.f = data.frame(N = nrow(heart.females),\n                   weight = swt::mean_sd(heart.females$weight),\n                   transplant = swt::count_perc(heart.females$event)\n)\n\ntab = rbind(tab, tab.f, tab.m)\n\ntab = as.data.frame(t(tab))\ncolnames(tab) = c(\"Overall\", \"Female\", \"Male\")\nrownames(tab) = c(\"Sample size N\", \"Body weight in kg, mean (SD)\",\n                  \"Transplant, count (%)\")\ntab\n\n\n\n\nTable 2: Descriptives.\n\n\n\nOverall\nFemale\nMale\n\n\n\n\nSample size N\n2000\n500\n1500\n\n\nBody weight in kg, mean (SD)\n76.0 (12.0)\n65.0 (9.9)\n79.7 (10.2)\n\n\nTransplant, count (%)\n1374 (68.7)\n384 (76.8)\n990 (66.0)\n\n\n\n\n\n\nAs expected, females had an average weight of approximately 65 kg; males around 80 kg. Among females, about 77% received a transplant, compared to 66% of males.\n\nTesting is futile\nI tested whether sex was independent of a heart transplant using Pearson’s \\(\\chi^2\\)-test for count data. I knew this was a bad idea, but since looking at variables individually is common in medical research, doing it one more time didn’t hurt.\n\n\nCode\nM = table(heart$event, heart$sex)\nnontpx.fmt = sprintf(\"%d (%.1f)\", M[1,], M[1,]/marginSums(M, margin = 2)*100)\ntpx.fmt    = sprintf(\"%d (%.1f)\", M[2,], M[2,]/marginSums(M, margin = 2)*100)\ntab.fmt = rbind(nontpx.fmt, tpx.fmt)\ntab = as.data.frame(tab.fmt)\nrownames(tab) = c(\"no transplant\", \"transplant\")\ncolnames(tab) = c(\"female\", \"male\")\ntab\n\n\n\n\nTable 3: Contingency table with counts (%) showing the relationship between sex and heart transplant status.\n\n\n\nfemale\nmale\n\n\n\n\nno transplant\n116 (23.2)\n510 (34.0)\n\n\ntransplant\n384 (76.8)\n990 (66.0)\n\n\n\n\n\n\n\n\nCode\nstats = chisq.test(M, correct = FALSE)\n\n\nI found that more females received a transplant than expected (384 versus 343.5), while fewer males received a transplant than expected (990 versus 1030.5). This provided strong evidence that sex was related to receiving a heart transplant (p &lt; 0.001).\nBut what was the issue? This simple analysis failed to capture the whole picture. Weight was related to both sex and the likelihood of receiving a heart transplant (Figure 2). Females typically have lower body weight, and heavier individuals are less likely to receive a heart transplant. Therefore, the body weight acts as a confounder. If I were unaware of body weight, I might incorrectly conclude that being female increases the likelihood of receiving a heart transplant. However, this was not the case and would only mislead us, as a heavy female would have a low chance of a transplant, while a lightweight male would have a higher chance.\n\n\n\n\n\nflowchart LR\n  A(Sex) --&gt; B(Likelihood of a heart transplant)\n  A &lt;--&gt; C(Body weight)\n  C --&gt; B\n\n\n Figure 2: Body weight is a confounder variable as it is associated with sex and also impacts the outcome of a heart transplant. \n\n\n\n\n\nIt’s a bird, it’s a plane, it’s a model!\nNext, I analyzed the data all at once using a logistic regression model. The results are shown in Table 4. Figure 3 shows the relationship between weight and the chance of a heart transplant.\n\n\nCode\ndd = datadist(heart)\noptions(datadist='dd')\n\nfit = lrm(event ~ sex + rcs(weight, 3), data = heart)\nswt::tidy_rmsfit(fit)\n\n\n\n\nTable 4: Results from a logistic regression model.\n\n\n\n\n\n\n\n\n\n\n\nInterquartile difference\nOdds ratio (95%-CI)\nChi-Square\nd.f.\np-value\n\n\n\n\nsex\n–\n–\n0.0138\n1\n0.91\n\n\nsex female\n–\n1.02 (from 0.76 to 1.36)\n–\n–\n–\n\n\nweight\n16.12 (68.07–84.19)\n0.49 (from 0.42 to 0.57)\n119\n2\n&lt; 0.001 ***\n\n\nweight nonlinear\n–\n–\n33.5\n1\n&lt; 0.001 ***\n\n\nTOTAL\n–\n–\n139\n3\n&lt; 0.001 ***\n\n\n\n\n\n\n\n\nCode\nav = anova(fit)\nggplot(Predict(fit, weight), anova = av, colfill = \"#6F87A6\", adj.subtitle = FALSE) +\n  xlab(\"Body weight (kg)\") +\n  theme_minimal() \n\n\n\n\n\nFigure 3: Partial effect of body weight and the chance of a heart transplant.\n\n\n\n\nIn conclusion, I found that body weight had a strong relationship with the chance of a heart transplant. Increasing weight from 68 kg to 84 kg was associated with roughly a 50% reduction in the chance of a heart transplant. No evidence was found for sex.\n\n\nSo what?\nA model is often the best description of the data. Regression models are part of university-level statistics courses across life sciences. Thus, there is really no excuse for testing variables individually instead of using a statistical model."
  },
  {
    "objectID": "posts/immortality/index.html",
    "href": "posts/immortality/index.html",
    "title": "Immortality",
    "section": "",
    "text": "Because I could not stop for Death,\nHe kindly stopped for me;\nThe carriage held but just ourselves\nAnd Immortality. —Emily Dickinson\nSurvival analysis is an important topic in medical statistics. Its most prominent method is the estimator developed by E. L. Kaplan and Paul Meier, and we will use the Kaplan–Meier estimator in the example at the end of this post.\nThe underlying assumption of survival analysis is a little bit sinister: we will all eventually experience death. However, for some of us, this event has not yet occurred, but it will happen in the future.\nIt reminds me to stay humble in medical data analysis. Being the person in charge of the data analysis does not place me outside the realities it describes. At the end of the day, we are all patients.\nAnyway—there is a problem in survival analysis that has always fascinated me, and it takes us in a slightly unexpected direction: immortality. It is interesting that patients can become immortal in data analysis. This artifact is known as immortal time bias.\nIn 2001, a study published in Annals of Internal Medicine claimed that Oscar winners lived almost 4 years longer than actors who were never nominated [1]. The study suffered from immortal time bias: in order to become an Oscar winner, one must survive long enough to actually win the Oscar. In the most extreme case, it was 79 years. The Oscar winners who were selected based on the fact that they had won an Oscar were not allowed to die; they were all immortal until they won. A critical discussion and reanalysis of the study was performed by Sylvestre [2], which is worth reading.\nThe reanalysis showed no evidence for a survival advantage of the Oscar winners."
  },
  {
    "objectID": "posts/immortality/index.html#what-is-immortal-time-bias",
    "href": "posts/immortality/index.html#what-is-immortal-time-bias",
    "title": "Immortality",
    "section": "What is immortal time bias?",
    "text": "What is immortal time bias?\nImmortal time bias occurs when survival beyond a certain time point is required in order to receive a treatment [3]. A textbook example is a study in which patients are assigned to the treatment group only if they fill a prescription for a corticosteroid within 90 days after hospital discharge (for example). To be classified as treated, patients must necessarily survive until the prescription is filled; this period is therefore immortal time.\nThe bias works in the direction that the treatment group will always appear to have better survival, even if the treatment has no true effect. This apparent benefit is not due to the medical intervention but is instead an artefact of a flawed study design. To avoid immortal time bias, patients must be assigned to treatment groups at time zero, or treatment must be modeled as a time-dependent exposure.\n\n\n\n\n\nflowchart LR\nA(Cohort entry) --&gt; B(Prescription filled)\nB --&gt; C(Treatment group)\nA --&gt; D(Control group)\nC --&gt; E[Event]\nD --&gt; E[Event]\n\n\n Figure 1: Diagram of a study design with immortal time in the period between cohort entry and drug prescription for the treatment group. \n\n\n\nOne of the earliest documented examples of immortal time bias was by Gail in 1972 in a study of heart transplantation [4]. But it keeps popping up still today."
  },
  {
    "objectID": "posts/immortality/index.html#a-recent-example-from-cancer-research",
    "href": "posts/immortality/index.html#a-recent-example-from-cancer-research",
    "title": "Immortality",
    "section": "A recent example from cancer research",
    "text": "A recent example from cancer research\nRecently, a LinkedIn post by Kaspar Rufibach caught my attention, pointing out suspected immortal time bias in a study published in Nature [5]. The study showed that in patients with advanced cancers (including non-small-cell lung cancer and metastatic melanoma), those who received an mRNA COVID-19 vaccine within 100 days of starting ICI treatment had substantially better overall survival.\nIn this paper, patients were classified as vaccinated only if they received a COVID-19 mRNA vaccine within a certain window after starting immune checkpoint inhibitor (ICI) therapy. Thus, this study could potentially suffer from immortal time bias, and it’s a very reasonable concern. To be counted as vaccinated, a patient must survive long enough after ICI initiation and remain well enough to receive vaccination.\nThis creates a period between ICI start (time zero) and vaccination during which death cannot occur for those later labeled “vaccinated,” but can occur for those labeled “unvaccinated.”\nThat period is, by definition, immortal time.\nImmortal time sneaks in when we forget that survival is not just about living longer, but about measuring it correctly. So, whether we are analyzing heart transplants, cancer treatments, or the longevity of Oscar winners, we must assess survival correctly.\nAnd leave true immortality to poetry.\nThe title of this post is a song by Pearl Jam. The preview image of this post is a photo by Jeremy Perkins on Unsplash."
  },
  {
    "objectID": "posts/immortality/index.html#just-one-more-thing",
    "href": "posts/immortality/index.html#just-one-more-thing",
    "title": "Immortality",
    "section": "Just one more thing",
    "text": "Just one more thing\nI simulated a simple dataset to illustrate immortal time bias using an intentionally extreme setup to make the concept clear. The dataset included 100 patients. Treatment was a time-dependent exposure, but it is incorrectly coded as a baseline indicator based on future information, specifically, whether a patient survived long enough to initiate treatment. This misclassification introduces immortal time, which in turn produces immortal time bias when treatment is analyzed as a baseline exposure.\nKey parameters of simulation:\n\nPatients: 200\nSurvival: exponential distribution, median was 2 years\nTreatment timing: beta distribution skewed toward later treatment, max. was 1 year\nTreatment indicator: defined at baseline using future survival, which created immortal time\nCensoring: uniform administrative censoring\n\n\n\nCode\nlibrary(survival)\nlibrary(ggsurvfit)\n\nset.seed(2026)\n\nn = 200\na = 2 # median survival time\nb = 1 # max. time until treatment\n\n# True survival\nlambda = log(2) / a\nsurv_time = rexp(n, rate = lambda)\n\n# Treatment timing\ntreat_time = 0.05 + rbeta(n, 5, 2) * b\n\n# Administrative censoring\ncensor_time = runif(n, 0.5, 2)\n\n# Observed data\ntime = pmin(surv_time, censor_time)\nstatus = as.integer(surv_time &lt;= censor_time)\n\n# Immortal-time-biased treatment indicator\ntreated = rep(\"control\", n)\ntreated[treat_time &lt; time] = \"treated\"\ntreated = as.factor(treated)\n\ndata = data.frame(\n  time = time,\n  status = status,\n  treated = treated\n)\n\n\nI then fitted a Kaplan-Meier survival curve for the treatment and control group.\n\n\nCode\n# Kaplan-Meier\nfit = survfit2(Surv(time, status) ~ treated, data = data)\n\nCOLORS = c(\"#BF505A\", \"#D9A282\", \"#818C70\", \"#B4BF8A\")\n\n# Plot\nggsurvfit(fit) +\n  labs(x = \"Follow-up time (in years)\",\n       y = \"Survival Probability\") +\n  add_confidence_interval() +\n  scale_color_manual(values = COLORS[c(1, 4)]) +\n  scale_fill_manual(values = COLORS[c(1, 4)]) +\n  theme_minimal()\n\n\n\n\n\nKaplan–Meier survival curves for treated and control groups. The apparent survival advantage in the treatment group is entirely due to immortal time bias, not a true treatment effect.\n\n\n\n\nThe analysis suggested longer survival in the treatment group, not because treatment was effective, but because some individuals were immortal by design.\n\nSo what?\nThis simulation showed that if you ignore the timing of treatment, it can look like a therapy works even when it doesn’t. Careful study design and analysis are needed to avoid being misled by immortal time bias."
  },
  {
    "objectID": "posts/try/index.html",
    "href": "posts/try/index.html",
    "title": "Try, try, try",
    "section": "",
    "text": "The root of the word “trial” is the Anglo-French word “trier,” meaning to separate out as in sorting the good from the bad or the fact from fiction. “Trial” is also closely related to the verb “try,” as both share the same roots. In medicine, the idea of a clinical trial comes from that same tradition: you’re testing an intervention, trying it out, putting it on trial.\nTen years ago, it was 2015. I analyzed a randomized controlled trial (RCT) evaluating the Attempted Suicide Short Intervention Program (ASSIP), a brief intervention designed for individuals with a history of suicide attempts. The results were striking: over a 24-month follow-up, ASSIP was associated with a significant reduction in repeat suicide attempts, yielding a hazard ratio of 0.17 (95% CI 0.07–0.46). This meant that patients in the intervention group had an approximately 80% lower reattempt rate compared to those receiving treatment as usual.\nThe study, published in PLOS Medicine in 2016 [1], gained international attention, including coverage in the Washington Post [2], and has since been cited more than 300 times.\nUnfortunately, subsequent studies could not replicate these findings, including a recently published report [3]. What causes the results of an early study not to be replicated in later studies?"
  },
  {
    "objectID": "posts/try/index.html#has-something-gone-wrong",
    "href": "posts/try/index.html#has-something-gone-wrong",
    "title": "Try, try, try",
    "section": "Has something gone wrong?",
    "text": "Has something gone wrong?\nWhen a study cannot be confirmed by independent researchers, some questions may arise. For example, was the statistical analysis done correctly in the original report?\nInterestingly, two years after the publication of the original study, our findings were independently confirmed by a prominent research group through a reanalysis of our data. The results were published in the British Medical Journal (BMJ) [4].\nOf course, one could always argue about alternative statistical approaches, as I had to make several analytical decisions at the time—some of which I might choose differently today, given what I’ve learned since. I have definitely become much more conservative throughout the years, and if I made a comprehensive reanalysis of the data, I would perhaps end up with a hazard ratio of 0.22 (95%-CI 0.08–0.59). The uncertainty is wider, but the point estimate is still an approximately 80% reduction of the suicide reattempt rate.\nNo matter how you slice it, it’s still the same pie.\nThese analytical choices were related to how missing data were addressed and how repeated events were handled. For a clinician, these might be statistical nuances, and the result at the core was clear as glass: in the ASSIP group, five patients made five repeat suicide attempts, while in the control group, 16 patients made a total of 41 attempts.\nHowever, even if a study’s statistical analysis was sound, replication with new data can still fail for a variety of reasons. First, I discuss factors that may play a role in the design of the replication study. Second, I will elaborate that even if both the original and replication study were performed in good faith, a replication study may still fail."
  },
  {
    "objectID": "posts/try/index.html#primary-outcome-secondary-disappointment",
    "href": "posts/try/index.html#primary-outcome-secondary-disappointment",
    "title": "Try, try, try",
    "section": "Primary outcome, secondary disappointment?",
    "text": "Primary outcome, secondary disappointment?\nHere are three key questions I find most relevant when the primary outcome failed [5], though the referenced paper discussed several others worth considering.\n\nWas the study sample size underpowered? When a replication study shows no effect—or, more precisely, finds no evidence for an effect—there’s often a temptation to spin this as definitive evidence of no effect. But to be convincing, a replication should ideally have a sample size at least as large as the original study, if not larger.\nWas exactly the same primary outcome used? A replication study should ideally use the same outcome as the original study. If the authors believe there is a better outcome measure, they should still use the original outcome for the primary analysis and report their choice of the outcome in a secondary analysis.\nWas the study population appropriate and accurately defined? Different patient mixes may render a treatment less effective. For example, if a treatment is most effective in severely ill patients or, conversely, only works in patients with mild symptoms.\n\nIn summary, when the primary outcome fails, scrutiny is needed to determine whether the study was adequately powered, used consistent outcomes, and recruited a comparable patient population. These factors can critically influence the interpretation of replication results. Without these careful considerations, we risk drawing misleading conclusions from seemingly null findings.\n\n\n\nAlberto Giacometti, “L’Homme qui marche II” (The Walking Man II) from 1960. Giacometti’s famously thin, elongated bronze sculptures reflect the shrinkage of the human form, often interpreted as existential fragility or the shrinking presence of the individual in the modern world. Photo: Louisiana Museum of Modern Art, Denmark"
  },
  {
    "objectID": "posts/try/index.html#pains-of-evidence",
    "href": "posts/try/index.html#pains-of-evidence",
    "title": "Try, try, try",
    "section": "Pains of evidence",
    "text": "Pains of evidence\nNow comes the sobering part. It’s a well-documented, almost inevitable pattern in clinical research that effect sizes are exaggerated in initial reports and tend to shrink in follow-up studies [6]. What once looked promising in an early trial often fades under the scrutiny of replication. It’s tempting to take this personally (as if science is betraying our hopes), but really, it’s just how the processes are. Initial studies often ride a wave of excitement, small samples, selective conditions, and the sheer luck of hitting statistical significance. What we thought was gold might still hold value, of course, but it rarely shines as bright the second time.\nAnd there are a few reasons for all this:\n\nThe winner’s curse: Early studies obtained a “statistically significant” result, which can be considered a “win.” However, as a consequence, one is cursed with an overestimation of the true effect.\nSmall sample sizes: Early studies are often underpowered, with small sample sizes. Subsequent larger studies with representative samples may shrink the effect estimate toward zero.\nRegression to the mean: If a treatment appears to have an unusually large effect in one trial, future studies are statistically more likely to show less extreme results. In other words, this is a basic principle of probability, and the extreme result was just random variation.\nImproved methodology: Later trials are often more rigorous: larger, better controlled, and designed to reflect real-world conditions. With cleaner protocols, reduced bias, and improved measurement, the effect estimates that emerge are typically smaller.\nDifferent populations or settings: Early studies may be done in very specific, idealized conditions: single centers, highly selected patient populations, and highly trained practitioners. Later, multicenter or international trials with more diverse, representative patients often see smaller, more generalizable effects.\n\nTaken together, these factors mean that shrinking effect estimates aren’t necessarily an unnatural phenomenon. Instead, that’s just what growing up looks like in the world of evidence-based medicine. Just like children growing up, they perceive the world gradually as less big and less exciting. The same may be true for researchers when looking subsequently at novel evidence.\nSometimes, we begin as fire and end as smoke.\n\nAnd promises are sweet. —R.E.M.\n\nBy the way, about 90% of all drugs fail during development [7,8].\nThe title of this post is a song by The Smashing Pumpkins."
  },
  {
    "objectID": "posts/try/index.html#just-one-more-thing",
    "href": "posts/try/index.html#just-one-more-thing",
    "title": "Try, try, try",
    "section": "Just one more thing",
    "text": "Just one more thing\nReplication power depends only on the original p-value and the sample size of the replication study relative to the original study [9,10]. Calculating statistical power across different relative sample sizes is possible using the ReplicationPower package. A relative sample size of 2 means the replication study has twice as many patients as the original study.\nFor that, I calculated the p-value for the reported hazard ratio and confidence interval in the original study, which was 0.17 (95% CI 0.07–0.46). First, I determined the standard error (SE) on the log scale.\n\\[\nSE = \\frac{log(0.46) - log(0.07)}{2 \\times 1.96} \\approx 0.4803\n\\]\nWith this, I calculated the z statistic.\n\\[\nz = \\frac{log(0.17)}{SE} \\approx -3.6894\n\\]\nThen, I looked up the p-value for this \\(z\\)-statistic.\n\\[\np = 2 \\times (1 - \\Phi(|z|)) \\approx 0.00022\n\\]\nFinally, I plotted the replication power dependent on different relative sample sizes in Figure 1.\n\n\nCode\nlibrary(ReplicationSuccess)\n\np = 0.00022\npower = seq(0.05, 0.95, 0.01)\nrelSampleSize = sampleSizeReplicationSuccess(zo = p2z(p), power = power,\n                                             designPrior = \"predictive\", shrinkage = 0.25)\n\nplot(relSampleSize, power, col = \"#BF505A\", type = \"l\", lwd = 1.5,\n     xlab = \"Relative sample size\", ylab = \"Replication power\", ylim = c(0, 1))\nabline(h = c(0.80, 0.90), col = \"#6F87A6\", lty = c(2, 3), lwd = 1.5)\n\n\n\n\n\nRelative sample sizes of the replication study against replication power. The dashed line shows a power of 80%, and the dotted line for 90%.\n\n\n\n\n\n\nCode\nrep_n = sampleSizeReplicationSuccess(zo = p2z(p), power = c(0.80, 0.90),\n                                     designPrior = \"predictive\", shrinkage = 0.25)\n\n\nUsing a significance level of 0.025 (corresponds to the standard two-sided test with an alpha of 0.05) and a shrinkage of 25% for the likely exaggerated effect estimate reported in the original study, I found that the relative sample sizes were 1.13 and 2.31 for a replication power of 80% and 90%, respectively.\nThus, when the original study had 120 patients, then the replication study should have 136 patients to achieve a replication power of 80% and 278 patients to achieve a replication power of 90%.\nThe Monn et al. study stated the following at the end of their methods section:\n\nA total sample size of n = 50 (i.e., 25 per group) would provide 80% power with an alpha level of 5% to detect effects at Cohen’s w = 0.4.\n\n\n\nCode\npower1 = powerReplicationSuccess(zo = p2z(p), c = 50/120,\n                                 designPrior = \"predictive\", shrinkage = 0.25)\npower2 = powerReplicationSuccess(zo = p2z(p), c = 92/120,\n                                 designPrior = \"predictive\", shrinkage = 0.25)\n\n\nI don’t believe that’s anywhere close to being enough. I calculated the replication power the way they planned the study, i.e., with 50 patients. The replication power was only 56%. However, in the end, they randomized N = 92 patients; this corresponded to a replication power of 72%.\nMonn et al. stated that “this non-replication is unlikely the result of low power or a small sample size.” By “non-replication,” I assume they mean that their replication study was unable to replicate the original study’s results, particularly the reported benefit of the treatment. However, I believe the replication study fell short in providing convincing power to make such a statement, and even the N = 92 did not reach the commonly accepted 80%, and dropouts are not accounted for.\nUltimately, if one truly aims to provide conclusive evidence on whether such a brief therapy is effective, it would be reasonable to plan for 90% power, requiring a sample size of N = 278. Accounting for an estimated 10% dropout rate, the total sample size should be approximately N = 308, over six times larger than the sample size originally proposed in the in the methods section of the replication study.\n\nSo what?\nThere is an urgent need for collaboration between medical statisticians and mental health professionals to properly design multi-center trials that can help identify effective interventions for individuals with a history of suicide attempts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Simon Schwab",
    "section": "",
    "text": "I’m a statistician at Swisstransplant with professional qualifications in medical research and statistics. I’m also a lecturer in biostatistics and good research practice at the Department of Biostatistics and the Center for Reproducible Science of the University of Zurich.\nI obtained my PhD in Health Sciences and a postgraduate certificate in Advanced Statistical Data Science from the University of Bern. I received 2.5 years of postdoctoral training in statistics at the University of Warwick and the University of Oxford, funded by the Swiss National Science Foundation (SNSF).\nFurthermore, I’m a member of the scientific committee of the Swiss Transplant Cohort Study (STCS) and a founding member of the interest group Statistics in Organ Transplantation (SOT)."
  },
  {
    "objectID": "about.html#memberships",
    "href": "about.html#memberships",
    "title": "About Simon Schwab",
    "section": "Memberships",
    "text": "Memberships\n\nMember of the Swiss Statistical Society (SSS)\nFellow and Graduate Statistician of the Royal Statistical Society (RSS)\nMember of the International Society for Clinical Biostatistics (ISCB)"
  },
  {
    "objectID": "about.html#private",
    "href": "about.html#private",
    "title": "About Simon Schwab",
    "section": "Private",
    "text": "Private\nI am married and the father of two children (born in 2021 and 2023). I enjoy playing guitar and listening to rock music.\n\n\n\nPearl Jam live in Zurich on 22. June 2022."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Fiction, future, and prediction",
    "section": "",
    "text": "I post about statistics and data in medical research. Every post ends with a section “Just one more thing” to provide a concrete data example with code in the R programming language that the reader can try out and examine.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImmortality\n\n\n\nImmortal time bias\n\nSurvival analysis\n\n\n\nImmortal time bias occurs when patients must survive a certain period to receive treatment, making them “immortal” during that time. This can falsely suggest a survival benefit. From Oscar winners to cancer patients, this subtle bias can distort results, reminding analysts that measuring survival accurately requires careful attention to time.\n\n\n\n\n\nJan 14, 2026\n\n\nSimon Schwab\n\n\n\n\n\n\n\n\n\n\n\n\nTry, try, try\n\n\n\nRandomized controlled trial\n\nReplication\n\n\n\nA randomized controlled trial is testing an intervention or trying it out. Unfortunately, it is not uncommon for subsequent studies to shrink initial effects towards zero, and there are reasons for that. The situation may simply highlight the natural evolution of scientific evidence in medicine, where initial excitement often gives way to more realistic assessments over time.\n\n\n\n\n\nMay 19, 2025\n\n\nSimon Schwab\n\n\n\n\n\n\n\n\n\n\n\n\nLand of confusion\n\n\n\nConfounding\n\nStatistical modeling\n\nMultivariable models\n\n\n\nConfounding can mislead medical research, obscuring true relationships between variables. This post explores how confounding biases arise, illustrating with real-world examples. Model-based approaches, like multivariable regression models, adjust for multiple confounders and offer a more accurate framework for interpreting complex data and making informed clinical decisions.\n\n\n\n\n\nMar 30, 2025\n\n\nSimon Schwab\n\n\n\n\n\n\n\n\n\n\n\n\nTrue colors\n\n\n\np-values\n\nResearch credibility\n\nResearch waste\n\n\n\nStatistical significance is often misunderstood. Even when a low p-value is obtained, it does not guarantee the result is true; in fact, it may still be wrong. This claim can be effectively illustrated using colors, revealing how many studies were really true and how many were false. Without skepticism and blindly focusing on the p-value while ignoring other factors, I might just as well believe in Santa Claus.\n\n\n\n\n\nDec 15, 2024\n\n\nSimon Schwab\n\n\n\n\n\n\n\n\n\n\n\n\n1901\n\n\n\nStatistical computing\n\nLinux\n\n\n\nWhile cleaning out my basement, I found a 20-year-old IBM computer. First, I wanted to get rid of it because the computer has become too old and useless. Instead, I installed a lightweight Linux operating system, R, Quarto, and RStudio. I then successfully analyzed the Titanic dataset as a showcase. It worked extremely well. Linux can bring new life to old computer hardware.\n\n\n\n\n\nOct 20, 2024\n\n\nSimon Schwab\n\n\n\n\n\n\n\n\n\n\n\n\nI don’t sleep, I dream\n\n\n\nAnimal studies\n\nRepeated inspection of data\n\np-values\n\n\n\nA 2018 sleep study increased its sample size until achieving statistical significance—a flawed practice known as p-hacking. This post explains why such practices are problematic, how they inflate false positives, and what that means for research credibility. Using data simulations, I highlight the pitfalls of data peeking and make the case for pre-specified statistical analysis plans in preclinical animal research.\n\n\n\n\n\nAug 18, 2024\n\n\nSimon Schwab\n\n\n\n\n\n\n\n\n\n\n\n\nFake plastic trees\n\n\n\nAI\n\nBig data\n\nBiomarkers\n\np-values\n\nUnivariable screening\n\n\n\nIn today’s data-driven world, more data doesn’t always mean better decisions. We often overvalue data and undervalue statistics. Excessive, poorly guided analyses, especially with high-dimensional data, risk overfitting and false discoveries. The post warns that data is meaningless without solid statistical methods and expert oversight, and cautions against trusting AI-driven analyses.\n\n\n\n\n\nJul 9, 2024\n\n\nSimon Schwab\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "output.html",
    "href": "output.html",
    "title": "Research output",
    "section": "",
    "text": "Schwab S, Iten F. Against all odds: Why a lung donor score does not add up. Transpl Int. 2025;38(14937):14937. doi:10.3389/ti.2025.14937\nSchwab S, Banz V, Held U, Hoessly L, Magini G. Does the UK DCD risk score have statistical flaws? J Hepatol. 2025;83(2):e84-e85. doi:10.1016/j.jhep.2025.04.030\n\n\n\nSchwab S, Elmer A, Sidler D, Straumann L, Stürzinger U, Immer F. Selection bias in reporting of median waiting times in organ transplantation. JAMA Netw Open. 2024;7(9):e2432415. doi:10.1001/jamanetworkopen.2024.32415\nSchwab S, Steck H, Binet I, et al. EXAM: Ex-vivo allograft monitoring dashboard for the analysis of hypothermic machine perfusion data in deceased-donor kidney transplantation. PLOS Digit Health. 2024;3(12):e0000691. doi:10.1371/journal.pdig.0000691\nvan Zwet E, Gelman A, Greenland S, Imbens G, Schwab S, Goodman SN. A new look at P values for randomized clinical trials. NEJM Evid. 2024;3(1):EVIDoa2300003. doi:10.1056/EVIDoa2300003\n\n\n\nSchwab S, Sidler D, Haidar F, et al. Clinical prediction model for prognosis in kidney transplant recipients (KIDMO): study protocol. Diagn Progn Res. 2023;7(1):6. doi:10.1186/s41512-022-00139-5\n\n\n\nSchwab S, Janiaud P, Dayan M, et al. Ten simple rules for good research practice. PLoS Comput Biol. 2022;18(6):e1010139. doi:10.1371/journal.pcbi.1010139\nFor more publications, see Google Scholar and Pubmed."
  },
  {
    "objectID": "output.html#publications",
    "href": "output.html#publications",
    "title": "Research output",
    "section": "",
    "text": "Schwab S, Iten F. Against all odds: Why a lung donor score does not add up. Transpl Int. 2025;38(14937):14937. doi:10.3389/ti.2025.14937\nSchwab S, Banz V, Held U, Hoessly L, Magini G. Does the UK DCD risk score have statistical flaws? J Hepatol. 2025;83(2):e84-e85. doi:10.1016/j.jhep.2025.04.030\n\n\n\nSchwab S, Elmer A, Sidler D, Straumann L, Stürzinger U, Immer F. Selection bias in reporting of median waiting times in organ transplantation. JAMA Netw Open. 2024;7(9):e2432415. doi:10.1001/jamanetworkopen.2024.32415\nSchwab S, Steck H, Binet I, et al. EXAM: Ex-vivo allograft monitoring dashboard for the analysis of hypothermic machine perfusion data in deceased-donor kidney transplantation. PLOS Digit Health. 2024;3(12):e0000691. doi:10.1371/journal.pdig.0000691\nvan Zwet E, Gelman A, Greenland S, Imbens G, Schwab S, Goodman SN. A new look at P values for randomized clinical trials. NEJM Evid. 2024;3(1):EVIDoa2300003. doi:10.1056/EVIDoa2300003\n\n\n\nSchwab S, Sidler D, Haidar F, et al. Clinical prediction model for prognosis in kidney transplant recipients (KIDMO): study protocol. Diagn Progn Res. 2023;7(1):6. doi:10.1186/s41512-022-00139-5\n\n\n\nSchwab S, Janiaud P, Dayan M, et al. Ten simple rules for good research practice. PLoS Comput Biol. 2022;18(6):e1010139. doi:10.1371/journal.pcbi.1010139\nFor more publications, see Google Scholar and Pubmed."
  },
  {
    "objectID": "output.html#r-software",
    "href": "output.html#r-software",
    "title": "Research output",
    "section": "R software",
    "text": "R software\n\n\ncochrane—Import Data from the Cochrane Database of Systematic Reviews (CDSR).\nEXAM—Ex vivo allograft monitoring dashboard.\nswt—Swisstransplant R package."
  },
  {
    "objectID": "output.html#reproducibility-notes",
    "href": "output.html#reproducibility-notes",
    "title": "Research output",
    "section": "Reproducibility Notes",
    "text": "Reproducibility Notes\nA series of commentaries highlight topics related to the production of robust, effective, and reproducible science; see the notes featured on the CRS website."
  },
  {
    "objectID": "output.html#presentations",
    "href": "output.html#presentations",
    "title": "Research output",
    "section": "Presentations",
    "text": "Presentations\nA complete list of slides from my presentations is available at OSF.\n\nTen simple rules for good research practice (link to video)—16/05/2024\nTalk at the Computational Reproducibility Seminar—NEXUS Personalized Health Technologies, ETH Zurich\n\nUnderstanding and managing bias in medical research—28/03/2024\nTalk for the Swiss Society for Pneumology—University Hospital of Zurich\nEXAM—Ex-vivo allograft monitoring dashboard for deceased-donor kidney transplantation (link to video)—21/06/2023\nTalk at R/Basel 2023—Roche Switzerland\n\nAn introduction to R with the Stanford Heart Transplant Data—09/12/2021\nTalk at BernR—Bern R user group\nFrom regression to machine learning in R (link to video)—15/04/2021\nTalk in the seminar Applied Machine Learning in Diagnostic Imaging—University Hospital of Zurich"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Research projects",
    "section": "",
    "text": "Below are my research projects, which include links to publications, study protocols, and presentations."
  },
  {
    "objectID": "projects.html#kidmokidney-prediction-model",
    "href": "projects.html#kidmokidney-prediction-model",
    "title": "Research projects",
    "section": "1. KIDMO—Kidney prediction model",
    "text": "1. KIDMO—Kidney prediction model\nKIDMO is a clinical prediction model for the prognosis in kidney transplant recipients at the time of organ offer. It will support decision-making to better understand donor, recipient, and transplant-related risks. The model is currently developed with data from over 2,000 kidney transplant recipients.\n\n\n\nFigure: Schematic overview of the KIDMO multivariable prediction model.\n\n\n\nResources\n\nStudy registration from 1. September 2022\nOnline risk calculator\nPoster presentation at the ISCB44 in Milano\n\n\n\nPublications\nSchwab S, Sidler D, Haidar F, et al. Clinical prediction model for prognosis in kidney transplant recipients (KIDMO): study protocol. Diagn Progn Res. 2023;7(1):6. doi:10.1186/s41512-022-00139-5"
  },
  {
    "objectID": "projects.html#examex-vivo-allograft-monitoring",
    "href": "projects.html#examex-vivo-allograft-monitoring",
    "title": "Research projects",
    "section": "2. EXAM—Ex vivo allograft monitoring",
    "text": "2. EXAM—Ex vivo allograft monitoring\nEXAM is an analytics dashboard for analyzing hypothermic machine perfusion data in deceased-donor kidney transplantation.\n\n\n\nFigure: EXAM analytics dashboard showing perfusion and temperature time series and various statistical indicators.\n\n\n\nResources\n\nEXAM online dashboard\nGitHub code repository\nTalk at R/Basel 2023 (Video)\n\n\n\nPublications\nSchwab S, Steck H, Binet I, et al. EXAM: Ex-vivo allograft monitoring dashboard for the analysis of hypothermic machine perfusion data in deceased-donor kidney transplantation. PLOS Digit Health. 2024;3(12):e0000691. doi:10.1371/journal.pdig.0000691"
  },
  {
    "objectID": "projects.html#waitwaitlist-analysis-in-transplantation",
    "href": "projects.html#waitwaitlist-analysis-in-transplantation",
    "title": "Research projects",
    "section": "3. WAIT—Waitlist analysis in transplantation",
    "text": "3. WAIT—Waitlist analysis in transplantation\nMedian organ waiting times published by transplant organizations may be biased when not appropriately accounting for censoring, death, and competing events. This can lead to overly optimistic waiting times for all transplant programs and, consequently, may deceive patients on the waiting list, transplant physicians, and healthcare policymakers. This study applied competing-risk multistate models to calculate probabilities for transplantation and adverse outcomes on the Swiss national transplant waiting list.\n\n\n\nFigure: Cumulative incidence curves for transplantation for the different organs A—E with 95% confidence bands and median time to transplantation (MTT) defined as the duration corresponding to the cumulative incidence of 0.50 (dashed line).\n\n\n\nResources\n\nWie lange muss man auf ein Spendeorgan warten? (German)\n\n\n\nPublications\nSchwab S, Elmer A, Sidler D, Straumann L, Stürzinger U, Immer F. Selection bias in reporting of median waiting times in organ transplantation. JAMA Netw Open. 2024;7(9):e2432415. doi:10.1001/jamanetworkopen.2024.32415"
  },
  {
    "objectID": "projects.html#reportreporting-and-evaluation-of-prediction-models-in-organ-transplantation",
    "href": "projects.html#reportreporting-and-evaluation-of-prediction-models-in-organ-transplantation",
    "title": "Research projects",
    "section": "4. REPORT—Reporting and evaluation of prediction models in organ transplantation",
    "text": "4. REPORT—Reporting and evaluation of prediction models in organ transplantation\nClinical prediction models for prognosis can predict outcomes and support decision-making. Previous research criticized the quality of prediction models concerning poor reporting and the risk of bias. How this applies to prediction models in organ donation and transplantation needs to be clarified. Therefore, this scoping review aims to assess prediction models used in transplant centers in Switzerland and update clinicians on the transparency, quality of reporting, and risk of bias of these tools.\n\n\n\nFigure: To transplant or not to transplant? Assessing the quality and limitations of existing prediction models can inform further research to develop novel or update existing models to improve the decision-making at transplant centers.\n\n\n\nResources\n\nStudy registration from 6. April 2024\n\n\n\nPublications\nSchwab S, Banz V, Held U, Hoessly L, Magini G. Does the UK DCD Risk Score have statistical flaws? J Hepatol. 2025;0(0). doi:10.1016/j.jhep.2025.04.030"
  },
  {
    "objectID": "projects.html#projects-as-a-statistical-consultant",
    "href": "projects.html#projects-as-a-statistical-consultant",
    "title": "Research projects",
    "section": "Projects as a statistical consultant",
    "text": "Projects as a statistical consultant\n\nHôpitaux Universitaires Genève (HUG)\nDonation-after-circulatory death liver transplantation. Outcomes and risk factors for graft loss and ischemic cholangiopathy in the Swiss setting.\nUniversitäts-Kinderspital Zürich\nPrediction of the serum creatinine after kidney transplantation in children.\nKantonsspital St. Gallen (KSSG)\nOutcome of dual kidney transplantation in Switzerland—a national cohort study.\nUniversity Hospital Zürich (USZ)\nDifferences between the observed and expected serum creatinine range after kidney transplantation."
  },
  {
    "objectID": "posts/fake/index.html",
    "href": "posts/fake/index.html",
    "title": "Fake plastic trees",
    "section": "",
    "text": "Where, oh, where would we researchers be without statistics? It enables us to analyze and report our study findings. Statistics add meaning to the data, creating information and value. Without statistics, data have only minimal meaning.\nMy view is that all data are rubbish without proper statistics. Nevertheless, the glorification of data has evolved over the last decades and continues to do so. An example is the front cover of the magazine Science from 2011. It is a collage of words, but only one stands out in large letters: “data.”\nWe are indeed very hungry for data. However, I don’t believe data should take center stage. What is more important is how we perform statistics on the data.\nI tried to find the word “statistics” on the Science front cover but could not find it. Can you see it? Maybe the word “data” should be a bit smaller to leave more room for the word “statistics,” the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.\nNevertheless, many medical studies are conducted with little or no involvement of adequately trained statisticians, and the same is true for scientific peer review [1]. Many researchers don’t use statistics properly. They might not know enough about it, or they might ignore expert advice. Statisticians are often only involved at the end of a study to analyze the data despite being specialists in helping researchers clarify the question and designing the study from scratch. Often, research projects do not include enough funding for statistical work.\nIt sounds absurd, but data is largely overvalued these days, and statistics is largely undervalued."
  },
  {
    "objectID": "posts/fake/index.html#big-data",
    "href": "posts/fake/index.html#big-data",
    "title": "Fake plastic trees",
    "section": "Big data",
    "text": "Big data\nAnother example of the glorification of data is the term Big Data.\nWhat does big data even mean? Often omics data is mentioned, for example, transcriptional, epigenetic, proteomic and metabolomic data [2].\nBig data may not be the latest advancement of biomedicine in tackling unsolved problems. Instead, it is our defeat against data-hungry methods from machine learning, a hyped discipline also known as artificial intelligence or simply “AI.” However, 4,000 biomarkers times 200 patients are not big data; the sample size is still N=200, and I consider this a small study. For a prediction model for prognosis, for example, I would instead prefer four clinically relevant variables from 2,000 patients, even though this reflects only 1% of the data in the former example. Don’t be too impressed by the width of a data set.\nToo much data can make data analysis harder, but this is often unrecognized. The unsystematic collection of high-dimensional data, when data sets are more “wide” than “long,” can lead to more questions than answers. For example, the failure of protein cancer biomarker research [3] or the failure of the Human Brain Project [4]. Learning from the past, the combination of big data and overhyped promises is a reason for concern.\nThe catchphrase “let the data speak for itself” is misleading because data rarely have meaning without context. To extract valuable insights, you need a good question and the appropriate method to answer it. Think about your favorite restaurant and dish: imagine being served only unprepared raw food. That’s what data are basically without a statistical analysis; it’s just the ingredients, the uncooked parts. Data on their own are inedible food. In fact, data on their own is pretty dumb.\nWhile data are dumb, more data can be even dumber. Let me explain this. You may object that more data will always lead to less uncertainty. This is, in principle, true. In reality, though, I can see lots of different ways that large data sets could be explored, analyzed, and reanalyzed until we get the result we want or until the model fits the data. Then, this process only appears to reduce the uncertainty when looking at the p-values and standard errors. More likely, the process by which the data were analyzed has increased our uncertainty, but this is often unrecognized. It is a fallacy called the garden of forking paths.\nExcessive analytic attempts generate phantom degrees of freedom [5], which is one of the many ways dark statistics can deceive us. We can only trust our statistical results if we don’t torture the data too much.\n\n\n\nGarden of Forking Paths, Parque Juan Carlos I, Madrid, Spain. (Photo by Victor on Unsplash)"
  },
  {
    "objectID": "posts/fake/index.html#brave-new-tools",
    "href": "posts/fake/index.html#brave-new-tools",
    "title": "Fake plastic trees",
    "section": "Brave new tools",
    "text": "Brave new tools\nStatistics summarize data into eatable bites. However, statistical data analysis is not something that can be prepared quickly and easily like a microwave-ready meal. It requires specialist work. And yes, mistakes can be made in this process [6]. Why can’t AI do all the difficult work for us?\nStatistical software once tried to create the illusion that a nice graphical interface could enable statistics at the push of a button. The exact opposite has happened with the R programming language. We moved away from pressing buttons to writing code, which, by the way, is reproducible and verifiable.\nI recently discovered a website with yet another doomed attempt to make statistics easy: an AI-powered statistics tool.\n\nIt makes complex data analytics accessible to anyone, eliminating the need for advanced technical knowledge or expertise. —from a website about an AI-based statistics tool\n\nHow does it work? I don’t know. Perhaps you can simply chat with your data files? For me, it sounds more like the production of flawed conclusions by untrained people via dangerous tools. That will be the gloomy future, it seems.\nIronically, the tool has the same name as an assassinated Roman emperor. This is not a very good omen for the tool. Maybe its future is already doomed, as we are all going to be very disappointed with AI doing our statistical analyses.\nMaybe, just maybe, AI will be terrible at being a personal statistical assistant. Maybe AI is much better at misusing statistics. It may be the perfect p-hacking machine. AI is misused in many ways to produce fake images, fake texts, fake human voices, and fake videos. Why not fake statistics? AI confuses the real with the unreal, and why can’t this also be applied to scientific data? The perfect tool for data fabrication in the future. Or, as Cathy O’Neil said, a new weapon of math destruction [7]. AI will do more harm than good, and in the future, we will need more people like Elisabeth Bik, who can spot fake research [8]."
  },
  {
    "objectID": "posts/fake/index.html#what-lies-ahead-of-us",
    "href": "posts/fake/index.html#what-lies-ahead-of-us",
    "title": "Fake plastic trees",
    "section": "What lies ahead of us?",
    "text": "What lies ahead of us?\nNowadays, AI and machine learning are heavily promoted to “revolutionize healthcare”; this seems an overhyped claim given the notorious overfitting of data in the field [9]. To be fair, it is not just AI that baffles me. Recently, 246 biologists obtained different results from the same data [10] or the latest blunder in functional MRI research [11]. This all demonstrates that we researchers seem to learn a lot from the noise in the data.\nNow, are you ready for what is next? A world where the relentless rush of technology blurs the lines between what’s real and what’s not?\nA world of fake plastic trees?\nThe title of this post is a song by Radiohead. The preview image of this post is a photo by Frames For Your Heart on Unsplash."
  },
  {
    "objectID": "posts/fake/index.html#just-one-more-thing",
    "href": "posts/fake/index.html#just-one-more-thing",
    "title": "Fake plastic trees",
    "section": "Just one more thing",
    "text": "Just one more thing\nI performed a simulation to highlight the challenges with wide data. First, I created data with many patients but only a few variables (long data). Say, the variables have been selected as the clinically most relevant prognostic variables based on the literature and expert knowledge.\nIn a second approach, I generated some high-dimensional data with many more variables than patients (wide data).\nBut first, I needed to write a short function tidy_lm() to produce a tidy output from a linear regression model; this is just cosmetics.\n\n\nCode\n# helper function for lm output\ntidy_lm &lt;- function(fit) {\n  \n  sfit =  summary(fit)\n  lower  = confint(fit)[,1]\n  upper  = confint(fit)[,2]\n  betas  = coef(fit)\n  effect = sprintf(\"%.2f\", betas)\n  ci     = sprintf(\"from %.2f to %.2f\", lower, upper)\n  \n  tab = data.frame(effect = effect,\n                   ci     = ci,\n                   pvalue = swt::tidy_pvalues(sfit$coefficients[,4]))\n  \n  colnames(tab) = c(\"Effect estimate\", \"95% CI\", \"p-value\")\n  return(tab)\n}\n\n\n\nLong data\n\n\nCode\nset.seed(2024)\n\nSNR = 0.65 # corresponds to 20% explained variance R^2\nN = 2000 # number of patients\nq = 4 # number of predictors\n\n# generate data for a study\ndata = array(rnorm(N*q), dim = c(N, q))\nnoise = rnorm(N) # generate standard normal errors\ny = 0.5*data[,1] + 0.3*data[,2] + 0.2*data[,3] + 1/sqrt(SNR)*noise\n\n\nI simulated data for \\(N = 2000\\) patients and \\(q = 4\\) variables. I assumed the three prognostic variables (\\(x_1\\), \\(x_2\\), and \\(x_3\\)) were uncorrelated and related to the outcome using an additive model. The true effects were \\(\\beta_1 = 0.5\\), \\(\\beta_2 = 0.3\\), and \\(\\beta_3 = 0.2\\). For the unrelated variable \\(x_4\\), we had \\(\\beta_4 = 0\\). I added some noise so that the explained variance was about 20% (\\(R^2\\)). Below are the results, see Table 1.\n\n\nCode\nd = data.frame(y = y, x1 = data[,1], x2 = data[,2], x3 = data[,3], x4 = data[,4])\nfit = lm(y ~ ., data = d)\ntidy_lm(fit)\n\n\n\n\nTable 1: Results from a regression model fitted with long data using simulated data from N=2000 patients.\n\n\n\nEffect estimate\n95% CI\np-value\n\n\n\n\n(Intercept)\n0.00\nfrom -0.05 to 0.06\n0.96\n\n\nx1\n0.49\nfrom 0.43 to 0.54\n&lt; 0.001 ***\n\n\nx2\n0.31\nfrom 0.25 to 0.37\n&lt; 0.001 ***\n\n\nx3\n0.23\nfrom 0.17 to 0.29\n&lt; 0.001 ***\n\n\nx4\n-0.01\nfrom -0.07 to 0.04\n0.67\n\n\n\n\n\n\nThe p-values for the first three variables were statistically significant. This basically means that the data were very incompatible with our null model that the true effects were zero (my lawyers helped me with the wording of this sentence). For \\(x_4\\), the p-value was statistically non-significant. But this is all not really interesting, is it? And I don’t particularly like to focus on the results in terms of statistical significance.\nMuch more interesting than the p-values were the effect estimates. Since I was the creator of the data, I could see that the estimates were close to the true effects, but not exactly. For the effect estimates to match the true effects even better, we would need even more data (tens of thousands of patients). But I think it was quite acceptable.\n\n\nWide data\n\n\nCode\nset.seed(2024)\n\nSNR = 0.65 # corresponds to 20% explained variance R^2\nN = 200 # number of patients\nq = 4000 # number of biomarkers\n\n# generate data for a study\ndata = array(rnorm(N*q), dim = c(N, q))\nnoise = rnorm(N) # generate standard normal errors\ny = 0.5*data[,1] + 0.3*data[,2] + 0.2*data[,3] + 1/sqrt(SNR)*noise\n\npvalues = rep(NA, q)\nfor (i in 1:q) { # iterate across biomarkers\n  d = data.frame (y = y, x = data[, i]) # univariable screening\n  fit = lm(y ~ x, data = d)\n  stats = summary(fit)\n  pvalues[i] = stats$coefficients[2,4]\n}\n\n\nNext, I increased the amount of data by a factor of 100. However, I did not increase the sample size in terms of the number of patients. Instead, I created data with \\(q = 4000\\) biomarkers (high-dimensional data). Since these may be expensive and complex measurements, I decreased the sample size to \\(N = 200\\). For each biomarker, I estimated a univariable model and used a cutoff of \\(p \\le 0.05\\) for screening (univariable screening). I know it is a really bad idea, but since so many papers have done it, it shouldn’t hurt anyone to do it just one more time.\nLet’s now pretend we knew that \\(x_1\\), \\(x_2\\), and \\(x_3\\) were the relevant biomarkers. The regression results from the same model as above (\\(x_1\\) to \\(x_4\\)) are shown below in Table 2.\n\n\nCode\nd = data.frame (y = y, x1 = data[,1], x2 = data[,2], x3 = data[,3], x4 = data[,4])\nfit = lm(y ~ x1 + x2 + x3 + x4, data = d)\ntidy_lm(fit)\n\n\n\n\nTable 2: Results from a regression model fitted with wide data using simulated data form N=200 patients.\n\n\n\nEffect estimate\n95% CI\np-value\n\n\n\n\n(Intercept)\n-0.05\nfrom -0.21 to 0.12\n0.58\n\n\nx1\n0.47\nfrom 0.31 to 0.63\n&lt; 0.001 ***\n\n\nx2\n0.20\nfrom 0.02 to 0.38\n0.026 *\n\n\nx3\n0.24\nfrom 0.07 to 0.41\n0.005 **\n\n\nx4\n0.07\nfrom -0.10 to 0.24\n0.41\n\n\n\n\n\n\nThe effect estimates were more off from the truth, particularly \\(x_2\\), and the confidence intervals were much wider. This was not surprising as the sample size was only \\(N = 200\\). All in all, we are much more uncertain; though, the p-values for \\(x_1\\) to \\(x_3\\) were still statistically significant. However, the larger uncertainty was not the main problem.\n\n\nCode\nk = sum(pvalues  &lt;= 0.05)\npvalues.adj = p.adjust(pvalues, method = \"fdr\")\nk.adj = sum(pvalues.adj  &lt;= 0.05)\n\n\nThe main problem was we didn’t know that \\(x_1\\), \\(x_2\\), and \\(x_3\\) were the relevant biomarkers. Using univariable screening, with \\(p \\le 0.05\\) for variable selection, a striking 214 out of a total of 4000 variables were statistically significant; this matches up with the alpha level of 5%.\nI tried to adjust the p-values to control the false discovery rate (FDR) at 5% using the Benjamini & Hochberg procedure; however, only \\(x_1\\) remained statistically significant. So, this only partially helped. I wanted more specificity, and I had to pay with sensitivity. It seems to be really difficult to get the correct model with such a large data set. It’s like looking for a needle in a haystack.\nIn the final step, perhaps out of desperation, I opted for a middle way: instead of using an unadjusted threshold of 5% or an FDR of 5%, I applied an unadjusted threshold of \\(p \\le 0.001\\). The results are shown in Table 3.\n\n\nCode\nidx = which(pvalues &lt;= 0.001) # select variables with arbitrary threshold\nd = data.frame(y = y, x = data[, idx]) \nfit = lm(y ~ ., data = d)\ntab = tidy_lm(fit)\n# add variable names\nrownames(tab)[2:(length(idx) + 1)] = paste0(\"x\", idx)\ntab\n\n\n\n\nTable 3: Results from a regression model fitted with wide data and using univariable screening to find relevant biomarkers.\n\n\n\nEffect estimate\n95% CI\np-value\n\n\n\n\n(Intercept)\n-0.01\nfrom -0.17 to 0.14\n0.86\n\n\nx1\n0.36\nfrom 0.20 to 0.51\n&lt; 0.001 ***\n\n\nx602\n-0.34\nfrom -0.50 to -0.17\n&lt; 0.001 ***\n\n\nx971\n0.19\nfrom 0.03 to 0.35\n0.017 *\n\n\nx2186\n0.28\nfrom 0.12 to 0.45\n&lt; 0.001 ***\n\n\nx3748\n0.14\nfrom -0.01 to 0.30\n0.073 .\n\n\n\n\n\n\nWith this step, we have left the realm of science facts and entered the realm of science fiction. The results above show voodoo statistics with statistically significant results for the variables that are pure noise (except \\(x_1\\)). The model explained 28% of the variance (adj. \\(R^2\\)).\nI think I tortured the data too much. Surely, a journal would publish such statistically significant results!\n\n\nSo what?\nSo what does this mean? I find the analysis of high-dimensional data extremely difficult. Certainly, there are situations where the problem is not so extreme, but there may be situations where the issue is even more extreme. It is almost impossible to find the relevant “truths” in the data when trying too many different things (the garden of forking paths).\nYou may object that the simulations I made were too pessimistic. Yes, but we often don’t know how good the signal-to-noise ratio is in clinical data. And completely in the dark, we must be very careful in the process of analyzing high-dimensional data, particularly when we use model selection. Univariable screening is well-known for providing flawed results, but it is still widely used today (or widely misused, I should say).\nThe process of how the result was obtained is much more important than the result itself.\nFor further reading, see “How to Do Bad Biomarker Research” by Frank Harrell [12]. For training, see the course materials “Analysis of High-Dimensional Data” by Nicolas Städler [13]."
  },
  {
    "objectID": "posts/colors/index.html",
    "href": "posts/colors/index.html",
    "title": "True colors",
    "section": "",
    "text": "Science has a high credibility and reputation. The public, the media, and politicians usually believe a scientific result is true. After all, it is science. If we don’t believe in science, what should we believe in? However, scientific findings are sometimes based on weak statistical results. I will focus on the widespread misunderstanding of the p-value.\nI would say it slightly differently. The vast majority of data analysis is performed by people who are not properly trained. Therefore, p-values are indeed used poorly as a consequence.\nThe poor understanding and misuse of p-values caused the American Statistical Association (ASA) to release a statement in 2016 [1]. The ASA argued that the p-values are commonly misinterpreted. Many researchers believe a low p-value is some form of insurance that the result or study is credible and worth publishing. A scientific result is worth publishing when the study was well planned, designed, conducted, and reported. A study protocol with a statistical analysis plan can help with this. The p-value in itself does not matter at all. In a perfect world, we see mostly high-quality studies with both low and high p-values. The opposite is true; there are too many low-quality studies, and too many studies report exclusively low p-values."
  },
  {
    "objectID": "posts/colors/index.html#what-does-the-p-value-actually-mean",
    "href": "posts/colors/index.html#what-does-the-p-value-actually-mean",
    "title": "True colors",
    "section": "What does the p-value actually mean?",
    "text": "What does the p-value actually mean?\nThe p-value is a measure of how compatible the data are with a specified model. This model is under the null hypothesis, which often says that there is no difference between two groups or there is an absence of a relationship between a variable and an outcome. The smaller the p-value, the greater this incompatibility of the data with the null hypothesis. Thus, a small p-value may then provide evidence against the null hypothesis.\nHere is how the ASA explained the p-value:\n\nInformally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. —ASA Statment\n\nIn other words, it is a probability statement about the data and not about the truth of a hypothesis. The most common misunderstanding is that a p-value of 0.012 reflects a 1.2% probability that the null hypothesis is true or that the data were produced by coincidence. These interpretations are not correct. Most researchers may not be aware that the commonly used alpha level of 5% is absolutely no guarantee that the result is not a fluke. In fact, p-values, the alleged gold standard of statistical validity, are not as reliable as many researchers believe [2]."
  },
  {
    "objectID": "posts/colors/index.html#a-lot-of-science-may-be-wrong",
    "href": "posts/colors/index.html#a-lot-of-science-may-be-wrong",
    "title": "True colors",
    "section": "A lot of science may be wrong",
    "text": "A lot of science may be wrong\nWhen looking at the body of, for example, medical research, I wonder, how many results are really true? How many results are not? Unfortunately, the answer may not be very amusing [3–5].\nTo get an approximate answer for a specific situation, I considered 1,000 studies; each was well-designed and conducted. I assumed the widely used alpha level was 5%, meaning that in 5 of 100 studies where there was no effect, I found a positive result. This is known as the type I error. Furthermore, I assumed all the studies had a statistical power of 80%. Thus, among the studies where there really was a true effect, I would find it in 80 of 100 studies. Consequently, in 20 out of 100 studies, however, I would miss the true result. These were the false negatives, also known as the type II error. My last assumption was about the question, how many hypotheses were really true? I assumed a study area where a good number of hypotheses turned out to be true, say, 3 in 10.\nImagine 1,000 hypotheses under investigation, where 3 in 10 were true. Thus, 300 were true, and 700 were false. Of the 700 false hypotheses, I found 5% to be falsely positive, which was 35. From the 300 true hypotheses, I found a positive result in 80% of the cases, which were 240. Altogether, I saw 35 and 240 results as positive results; however, 35 were false, which was 13%.\nI used the common alpha level of 5%, but 13% of the results were wrong. This really is the best-case scenario with an alpha level of 5% and a statistical power of 80%. The reality is much worse.\nIn reality, the reported standard errors and p-values are often biased. The reason is that data is often reanalyzed until the desired result was obtained. Thus, many reported results probably have a much higher p-value than if data were analyzed according to a prespecified protocol. The 5% alpha level is maybe 10% or higher. Furthermore, the statistical power of 80% is from a perfect world, and this assumption is not realistic at all, even in the domain of the highest standards, such as clinical trials [6]. I don’t know what the true statistical power might be; maybe it is as low as 50%. Lastly, there are domains where true hypotheses are not so prevalent, for example, where only 1 in 10 hypotheses are, in fact, true.\nMaking a scientific claim in a domain where novel findings are hard to discover, where p-hacking is prevalent, and where sample size calculations and statistical power are dismissed makes a result very unlikely to be true, even if the p-value is statistically significant.\nWhen I look at a p-value, I don’t see it solely in terms of whether it is statistically significant. I put it in some context and ask myself, can I believe a finding when taking other information into the equation, for example, the study design, the sample size, the statistical analysis, and how results have been reported? If I had no skepticism and just looked at the magnitude of a p-value, ignoring everything else, I might just as well believe in Santa Claus.\nOr let me take a better example. Ignoring the story behind a p-value and taking it at face value would be like waking up one morning with a headache and concluding that you have a rare brain tumor [2].\nA brain tumor may be possible but very unlikely. So are the results of many scientific discoveries.\nThe title of this post is a song by Cindy Lauper. The preview image of this post is a photo by Drew Beamer on Unsplash."
  },
  {
    "objectID": "posts/colors/index.html#just-one-more-thing",
    "href": "posts/colors/index.html#just-one-more-thing",
    "title": "True colors",
    "section": "Just one more thing",
    "text": "Just one more thing\nBelow, I used some plots to illustrate that p-values are not as reliable as many researchers believe. In these plots, each square was a hypothesis that was either true or false. Each hypothesis was tested by a study. Based on some assumptions about the proportions of true hypotheses, the alpha level, and the statistical power, I determined how likely a scientific result may be. This approach was strongly inspired by an article and video by the Economist [7].\n\n\nCode\nlibrary(ggplot2)\nlibrary(reshape2)\n\nk = 1000\n\nCOLORS = c(\"#BF505A\", \"#D9A282\", \"#818C70\", \"#B4BF8A\")\n\n\n\n\nCode\n# k: number of hypotheses\n# p: prop. of true hypotheses\n# alpha: alpha level\n# power: statistical power\n\ntrue_colors &lt;- function(k, p_true, alpha, power) {\n  \n  t = p_true*k\n  f = k - t\n  M = matrix(1:k, 20)\n  data = melt(M, varnames = c(\"row\", \"col\"), value.name = \"id\")\n  \n  data$H = \"false\"\n  data$H[1:t] = \"true\"\n  \n  # true and false hypotheses\n  data$H = factor(data$H, levels = c(\"false\", \"fn\", \"true\", \"fp\"))\n  \n  # add false pos and false neg\n  data$H_ = data$H\n  \n  fp = round(f*alpha)\n  idx = sample(which(data$H_ == \"false\"), fp)\n  data$H_[idx] = \"fp\"\n  \n  fn = round(t*(1 - power))\n  idx = sample(which(data$H_ == \"true\"), fn)\n  data$H_[idx] = \"fn\"\n  \n  pos = sum(data$H_ == \"true\")\n  neg = sum(data$H_ == \"false\")\n  fp  = sum(data$H_ == \"fp\")\n  fn  = sum(data$H_ == \"fn\")\n  fdr = fp / (pos + fp)\n  \n  return(list(data = data, k = k, p_true = p_true, alpha = alpha, power = power,\n              pos = pos, neg = neg, fp = fp, fn = fn, fdr = fdr))\n  \n}\n\nset.seed(2024)\nmylist = true_colors(k = k, p_true = 0.30, alpha = 0.05, power = 0.80)\n\n\nI considered 1000 hypotheses. I assumed that 30% of the hypotheses were true, see Figure 1.\n\n\nCode\nggplot(mylist$data, aes(x=col, y=row, fill=H)) +\n  geom_tile(color = \"gray90\") +\n  coord_equal() +\n  theme_void() +\n  scale_fill_manual(values = COLORS[c(1, 3)], guide = guide_none())\n\n\n\n\n\nFigure 1: Visualization of 1,000 hypotheses, of which 300 were true (green) and 700 were false (red).\n\n\n\n\nImagine each hypothesis was tested by a study. Given an alpha level of 0.05, there were 35 studies that produced a false positive result (type I error). The studies also had a power of 80%; thus, 60 of the studies were false negatives (type II error), see Figure 2.\n\n\nCode\nggplot(mylist$data, aes(x=col, y=row, fill=H_)) +\n  geom_tile(color = \"grey90\") +\n  coord_equal() +\n  theme_void() +\n  scale_fill_manual(values = COLORS, guide = guide_none())\n\n\n\n\n\nFigure 2: Visualization of studies that tested 1,000 hypotheses. Some studies produced a false positive result among the false hypotheses (light green). Some studies produced a false negative result among the true hypotheses (light red).\n\n\n\n\nIn reality, we see positive results, and we don’t know which are false. There were 240 positive results (green) and 35 false positive results (light green). The proportion of false positive results among all the positive results was 0.13; this is also known as the false discovery rate (FDR). Note that this error rate is more than double as high as the convenient alpha level of 0.05.\nHowever, the assumptions above really came from a perfect world. I then used a more realistic setting, for example, assuming a scientific field where 1 in 10 hypotheses was true, with an alpha level of 0.10 and a statistical power of 50%.\n\n\nCode\nset.seed(2024)\nmylist = true_colors(k = k, p_true = 0.10, alpha = 0.10, power = 0.50)\n\n\n\n\nCode\nggplot(mylist$data, aes(x=col, y=row, fill=H_)) +\n  geom_tile(color = \"grey90\") +\n  coord_equal() +\n  theme_void() +\n  scale_fill_manual(values = COLORS, guide = guide_none())\n\n\n\n\n\nFigure 3: Visualization of studies that tested 1,000 hypotheses. Some studies produced a false positive result among the false hypotheses (light green). Some studies produced a false negative result among the true hypotheses (light red).\n\n\n\n\nIn such a scenario, there were 50 positive results (green) and 90 false positive results (light green). Thus, 64% of all positive results were wrong. That was more than half of all the studies.\n\nSo what?\nMany research results may be wrong. Only trust studies that were well designed, conducted, analyzed, and reported. Don’t take the p-value as a measure of credibility or importance of a result. By the way, the reader can use the function true_colors() to try out different scenarios."
  },
  {
    "objectID": "posts/dream/index.html",
    "href": "posts/dream/index.html",
    "title": "I don’t sleep, I dream",
    "section": "",
    "text": "Sleep is fascinating. We all sleep at night, but we don’t really know what is happening during this period. Sometimes we dream, sometimes we don’t. All in all, it is a bit of a mystery.\nFor some researchers, statistics is also a bit of a mystery. In 2018, a study titled “Regulation of REM and Non-REM Sleep by Periaqueductal GABAergic Neurons” was published in Nature Communications led by researchers from the University of California at Berkeley, Stanford University, and the University of Pennsylvania [1]. The study investigated rapid eye movement (REM) sleep in mice, which is a sleep phase characterized by vivid dreams and random movement of the eyes.\nThe only thing I know about R.E.M. is that it is also the name of a pretty good rock band; I grew up listening to their music. The very first album I bought as a kid was The Best of R.E.M. from 1991. For me, rock music and statistics go hand in hand, just like in Andy Field’s book Discovering Statistics Using R [2]. But now, let’s get back to our sleep study.\nThe good news is that we don’t need to understand anything about REM sleep, the brain, or the mouse. I probably don’t even know the difference between a mouse, a rat, and a hamster. The subject matter would only distract us. Let’s ignore everything in this paper except a tiny bit from the methods section.\nHave you ever read just one sentence from a research study? It can be enough to make a case.\nBut first, let me say something else. In some prestigious journals, the methods and statistics section, the most relevant part for judging the validity and credibility of a study, is literally the small print at the end. This reminds me of what I wrote last time, that statistics is largely undervalued. Does anyone read that part?\nWell, I do."
  },
  {
    "objectID": "posts/dream/index.html#peeping-tom",
    "href": "posts/dream/index.html#peeping-tom",
    "title": "I don’t sleep, I dream",
    "section": "Peeping Tom",
    "text": "Peeping Tom\nDuring my time at the University of Warwick in Coventry, I learned about the legend of “Peeping Tom.” In my version, however, Peeping Tom was a scientist obsessed with observing the p-value. The scientist would always take a glimpse at that darn p-value and how it changed over the course of the experiment as more data was added. In the end, the scientist was struck blind—end of the story.\nLooking at the data again and again (and again) as more data are added and a statistical test is run each time, it is essentially a multiple-testing situation that would require some form of adjustment of the p-value. The reason is that this (bad) research practice considerably increases the chance that a statistically significant finding is a fluke.\nMoreover, the study authors failed to report the non-significant results from all the statistical tests they conducted. They continued testing until they found a significant result, which they then reported. This selective reporting, based on the significance of the test, skews the interpretation of their findings. Despite the ongoing debate about when to adjust p-values, the situation here clearly calls for a “haircut” to their p-values [3].\nIn the Seven Sins in statistical significance testing [4], the practice of repeatedly looking at the p-value is described as the sin number three, the repeated inspection of data.\nThe solution to this problem is simple. It can be avoided by not analyzing the data until the study is completed and the planned sample size is reached. But I’m not sure that sample size is often planned in animal research."
  },
  {
    "objectID": "posts/dream/index.html#seven-mice",
    "href": "posts/dream/index.html#seven-mice",
    "title": "I don’t sleep, I dream",
    "section": "Seven mice",
    "text": "Seven mice\nIn 2017, I applied for a grant for a neuroscience project that would analyze a large dataset of thousands of magnetic resonance imaging (MRI) scans. I was shortlisted for the second round, and after my presentation, a member of the committee asked me why I would not do my own MRI experiment to collect my own data. I replied that I did not want to do another N = 30 MRI study. The person replied that there were studies published in the journal Nature with only 7 mice. I had to laugh because I thought it was a joke, but nobody else was laughing.\nWell, here we are, years later, thinking more deeply about this. In the present study, the sample size was not 7 mice. It was 12 mice, to be fair.\n\n\n\nAndy Warhol, Myths: Mickey Mouse from 1981 (Halcyon Gallery)\n\n\nIn 2011, two comments about preclinical cancer research raised eyebrows [5,6]. Apparently, several landmark studies in high-impact journals that generated hundreds of secondary publications could not be verified by the pharmaceutical companies Bayer and Amgen. In these studies, investigators sometimes reported a single finding that supported their hypothesis, but it was often cherry-picked. This was the starting point for Tim Errington’s investigation of reproducibility in preclinical cancer research [7]. Reasons for irreproducibility in preclinical animal studies can include small sample sizes and thus low statistical power, as well as over-standardization of experiments [8]. After reading all all this, I’m not so surprised when another lab can’t confirm an exciting study finding from an animal experiment in a prestigious journal.\nIf a result cannot be repeated, what is it worth?\nReproducible animal studies require transparent reporting. The ARRIVE (Animal Research: Reporting of In Vivo Experiments) guidelines were initially developed in 2010 to improve the reporting of animal research, and version 2.0 was published in 2020 [9]."
  },
  {
    "objectID": "posts/dream/index.html#an-exception-to-the-rule",
    "href": "posts/dream/index.html#an-exception-to-the-rule",
    "title": "I don’t sleep, I dream",
    "section": "An exception to the rule",
    "text": "An exception to the rule\nThere are some exceptions where stopping a study early may be justified. But that’s a whole different ballgame. Such situations need to be predefined, and adequate statistical methods need to be used in the analysis. In clinical trials there is the term “interim analysis”, where an analysis is conducted before the end of the study and data collection. This can be very useful when a treatment is clearly beneficial or harmful compared to the control arm [10]. Then, the investigator can stop the study early. However, as I said, this is all very well planned and defined a priori in a study protocol and statistical analysis plan.\nFor example, the Cardiac Arrhythmia Suppression Trial (CAST) was a four-arm trial of three antiarrhythmic drugs versus placebo on the risk of sudden cardiac death [11]. Early in the CAST study, two arms (encainide and flecainide) were stopped because of excess mortality compared with the placebo arm.\nIn the present mouse study, however, the authors stopped the experiment because the p-value fell below the magical level of \\(p \\le 0.05\\), which apparently signaled to them that they could publish their results and didn’t need to kill any more mice. If you do enough tests, you will always obtain a p-value less than the magical threshold.\n\nI’m looking for an interruption. Do you believe? —I don’t sleep, I dream (song by R.E.M.)\n\nI don’t believe in stopping this study early, and I don’t trust the results. However, I believe that the authors of the study did not want to cheat. They probably didn’t know any better. They thought the approach was okay, so they wrote it in the methods section. No statistician was on the list of authors, I guess, and no statistician reviewed the paper.\nOne last thought. When the authors of the study stopped the experiment, it was good for the next mouse whose life was saved. Unless the study was flawed.\nThen all those mice would have been wasted.\nThe title of this post is a song by R.E.M.; the preview image of this post is a photo by Karsten Winegeart on Unsplash."
  },
  {
    "objectID": "posts/dream/index.html#just-one-more-thing",
    "href": "posts/dream/index.html#just-one-more-thing",
    "title": "I don’t sleep, I dream",
    "section": "Just one more thing",
    "text": "Just one more thing\n\n\nCode\nset.seed(2024)\nN = 30 # number of mice\nstop = 10:N # stop rules\nk = 10000 # number of studies\n\nps = array(NA, dim = c(k, length(stop)))\n\n# iterate across studies s\nfor (s in 1:k) {\n  \n  y1 = rnorm(N)\n  y2 = rnorm(N)\n  \n  c = 1\n  for (i in stop) {\n    \n    stat = wilcox.test(y1[1:i], y2[1:i], paired = TRUE)\n    ps[s, c] = stat$p.value\n    c = c + 1\n    \n  }\n}\n\n\nI conducted a mouse study. I assumed that the planned sample size was \\(N = 30\\) animals. However, I peeped at the data already after testing 10 animals and then repeated the test after each additional animal until \\(p \\leq 0.05\\) or the total sample size was reached. Since no animals were harmed in my experiment, I performed \\(k = 10000\\) of such studies.\nIn the mouse study, two measurements were obtained for each mouse. I sampled both observations from a normal distribution with a mean of 0 and a standard deviation of 1. Thus, the model behind the simulation assumed that there was no difference between the mice before and after exposure. Thus, any result I found was a fluke.\nI performed the same test reported in the paper, a Wilcoxon signed-rank test, but that doesn’t really matter for making the case. You could make the case with any statistical significance test that successfully controls the type I error at 5% (falsely rejecting the null).\nIn total, I collected \\(21 \\times 10^{4}\\) p-values (21 repeated inspections and 10000 studies).\n\n\nCode\nFP = sum(ps[,ncol(ps)] &lt;= 0.05)\n\n\nI looked at the last p-value for each study, i.e., after data from the total of \\(N = 30\\) animals had been collected. I found that 452 out of 10000 studies were statistically significant, which is a type I error rate of 0.045. Thank God, the error was under control. So far, so good.\n\n\nCode\n# function peep() to get p value using repeated inspection\npeep &lt;- function(x) {\n  \n  d = data.frame(t = NA, p = NA)\n  \n  # pick first p value that is below cutoff\n  if (min(x) &lt;= 0.05) {\n    i = which(x &lt;= 0.05)[1] # pick first p value that is below cutoff\n    d$t = i\n    d$p = x[i]\n    \n    # if no success pick last p value\n  } else {\n    d$t = length(x)\n    d$p = x[length(x)]\n  }\n  \n  return(d)\n}\n\nmyp = apply(ps, 1, peep)\nmyp = as.data.frame(t(array(unlist(myp), dim = c(2, k))))\ncolnames(myp) = c(\"t\", \"p\")\n\n\nNow, the twist. I peeped at the other p-values. I picked the p-value as soon as it turned statistically significant and stopped the experiment. But when I was not lucky enough to observe a \\(p \\leq 0.05\\), I picked the p-value from the last repeated test with \\(N = 30\\) animals, whatever that value looked like.\n\n\nCode\nFP = sum(myp$p &lt;= 0.05)\n\n\nI found that 1637 out of 10000 studies were statistically significant, which was a type I error rate of 0.16. Now, we have lost control. The type I error of falsely rejecting the null has tripled.\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nN_plots = 2\n\n# pick two significant studies\nset.seed(2024)\nidx = which(myp$p &lt;= 0.05)\nidx = sample(idx, N_plots)\n\np = list()\n\nfor (i in 1:N_plots) {\n  \n  d = data.frame(p = ps[idx[i],], stop = stop)\n  \n  p[[i]] = ggplot(d, aes(x = stop, y = p, col = \"red\")) + \n    geom_line() + \n    geom_point(size = 2) +\n    geom_point(data = myp[idx[i],], \n               size = 2,\n               mapping = aes(x = t + stop[1] - 1,\n                             y = p, col=\"green\")) +\n    ylim(c(0, 1)) +\n    geom_hline(yintercept = 0.05) +\n    scale_color_manual(values = c(\"#BF505A\", \"#6F87A6\")) +\n    labs(title = paste(\"Study\", i), tag = LETTERS[i]) +\n    xlab(\"Number of mice observed\") + ylab(\"p-value\") +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n}\n\ndo.call(\"grid.arrange\", c(p, nrow = 1, ncol=2))\n\n\n\n\n\nFigure 1: Two example studies that were stopped early due to peeping at the p-value before the study was completed. The red data point highlights the statistically significant p-value at which the study was stopped early. In (A), the study was stopped after observing 10 mice and (B) after observing 25 mice. The horizontal line shows the significance level of 0.05. As we can see, the p-value is very wiggly and can be exploited by bad research practices such as the repeated inspection of data.\n\n\n\n\nTwo case studies from the 1637 positive findings that were statistically significant by peeping are shown in Figure 1. Study 1 was statistically significant at the first glimpse, so we could already go home after 10 mice. Brilliant. Study 2 was statistically significant after 25 mice. Looking only at the last p-value, after the 30 mice, then both Study 1 and Study 2 were not statistically significant. This result would be correct since I was the creator of the data and knew the truth behind the data.\nThe truth was that there was nothing there.\n\nSo what?\nFirst, a p-value from repeated inspection of data is nothing more than a fluke of the universe. Don’t trust it. Second, Nature Communications might consider involving expert statisticians in the evaluation of a study. After all, their article processing fee is 6790 US dollars."
  },
  {
    "objectID": "library.html",
    "href": "library.html",
    "title": "Library of medical statistics",
    "section": "",
    "text": "Below is a curated list of essential books, compiliations, guidelines, journal collections, and other online resources that have proven to be highly valuable."
  },
  {
    "objectID": "library.html#books",
    "href": "library.html#books",
    "title": "Library of medical statistics",
    "section": "Books",
    "text": "Books\n\nGeneral\n\nBland M. An Introduction to Medical Statistics. 4th ed. Oxford University Press; 2015.\nChan BKC. Biostatistics for Epidemiology and Public Health Using R. Springer Publishing; 2015.\nDevore J, Peck R. Statistics: The Exploration & Analysis of Data. 7th ed. Cengage Learning; 2010.\nField A, Miles J, Field Z. Discovering Statistics Using R. SAGE Publications; 2012.\nHeld L, Rufibach K, Seifert B. Medizinische Statistik: Konzepte, Methoden, Anwendungen. 1st ed. Pearson; 2013.\nKirkwood BR, Sterne JAC. Essential Medical Statistics. 2nd ed. Blackwell Science; 2003.\nRosner B. Fundamentals of Biostatistics. 7th ed. Cengage Learning; 2010."
  },
  {
    "objectID": "library.html#compilations",
    "href": "library.html#compilations",
    "title": "Library of medical statistics",
    "section": "Compilations",
    "text": "Compilations\n\nCatalogue of Bias\nEQUATOR Network and reporting guidelines"
  },
  {
    "objectID": "library.html#guidelines",
    "href": "library.html#guidelines",
    "title": "Library of medical statistics",
    "section": "Guidelines",
    "text": "Guidelines\n\nICMJE Recommendations"
  },
  {
    "objectID": "library.html#journal-collections",
    "href": "library.html#journal-collections",
    "title": "Library of medical statistics",
    "section": "Journal collections",
    "text": "Journal collections\n\nBland & Altman. Statistics Notes. BMJ.\nSchwab & Held. Reproducibilty Notes. Significance.\nSedgwick P. Endgames. BMJ.\nGuide to Statistics and Methods. JAMA.\nStatistics at Square One. BMJ.\nStatistics for Biologists. Nature."
  },
  {
    "objectID": "library.html#podcasts",
    "href": "library.html#podcasts",
    "title": "Library of medical statistics",
    "section": "Podcasts",
    "text": "Podcasts\n\nThe Effective Statistician\nJAMA Guide to Statistics and Methods"
  },
  {
    "objectID": "library.html#software",
    "href": "library.html#software",
    "title": "Library of medical statistics",
    "section": "Software",
    "text": "Software\n\nThe R Project for Statistical Computing\nRStudio Desktop\nQuarto\nGit version control system\nRTools"
  }
]