---
title: "I don't sleep, I dream"
draft: false
description: "A 2018 study reported in the methods section that the number of mice was continuously increased until statistical significance was reached. It is very easy to show that such approaches are flawed and belong to the realm of dark statistics because they are more likely to produce a false positive result."
author: "Simon Schwab"
date: "2024-08-18"
categories: [Animal Studies, Repeated inspection of data, p-values]
image: "karsten-winegeart-4PWkY4HILj0-unsplash.jpg"
citation:
  issued: 2024
  type: post-weblog
  url: https://www.statsyup.org/posts/dream/
bibliography: references.bib
google-scholar: true
csl: plos.csl 
title-block-banner: true
df-print: kable
---

Sleep is fascinating. We all sleep at night, but we don't really know what is happening during this period. Sometimes we dream, sometimes we don't. All in all, it is a bit of a mystery.

For some researchers, statistics is also a bit of a mystery. In 2018, a study titled "Regulation of REM and Non-REM Sleep by Periaqueductal GABAergic Neurons" was published in *Nature Communications* led by researchers from the University of California in Berkeley, Stanford University, and the University of Pennsylvania [@Weber2018-fp]. The study investigated rapid eye movement (REM) sleep in mice, which is a sleep phase characterized by vivid dreams and random movement of the eyes.

The only thing I know about R.E.M. is that it is also the name of a pretty good rock band; I grew up listening to their music. The very first album I bought as a kid was *The Best of R.E.M.* from 1991. For me, rock music and statistics go hand in hand, just like in Andy Field's book *Discovering Statistics Using R* [@Field2012-hs]. But now, let's return to our sleep study.

The good news is we do not need to understand anything about REM sleep, the brain, or the mouse. I probably don't even know the difference between a mouse, a rat, and a hamster. The subject matter would only distract us. Let's ignore everything in this paper except a tiny bit from the methods section.

Have you ever read only a single sentence from a research study? It may be enough to make a case.

But first, let me say another thing. In some prestigious journals, the methods and statistics section, the most relevant part for judging the validity and credibility of a study, is literally the small print at the end. Reminds me of what I wrote last time, that [statistics is largely undervalued](https://www.statsyup.org/posts/fake/). Does anybody read that part?

Well, I do.

> "we continuously increased the number of animals until statistical significance was reached to support our conclusions." ---p. 12, the small print

# Peeping Tom

During my time at the University of Warwick in Coventry, I learned about the [legend of "Peeping Tom."](https://en.wikipedia.org/wiki/Lady_Godiva#Peeping_Tom) In my version, however, Peeping Tom was a scientist obsessed with observing the p-value. The scientist would always take a glimpse at that darn p-value and how it changed throughout the course of the experiment when adding more data. In the end, the scientist was struck blind---end of the story.

When looking at the data again and again (and again) as more data are added and a statistical test is run each time, it essentially is a multiple-testing situation that would require some form of adjustment of the p-value. The reason is that this (bad) research practice considerably increases the chance that a statistically significant finding is a fluke.

Moreover, the study authors failed to report the non-significant results from all the statistical tests they conducted. They continued testing until they found a significant result, which they then reported. This selective reporting, based on the significance of the test, skews the interpretation of their findings. Despite the ongoing debate about when to adjust p-values, the situation here clearly calls for a "haircut" for their p-values [@Boulesteix2024-kg].

In the *Seven Sins* in statistical significance testing [@Held2020-yx], the practice of repeatedly looking at the p-value is described as the sin number three, the *repeated inspection of data*.

The solution to this issue is straightforward. It can be avoided by not analyzing the data before the study has been completed and the planned sample size has been reached. But I'm not sure if sample size is often planned in animal research.

# Seven mice

In 2017, I applied for a grant for a project that planned to analyze a large dataset with thousands of magnetic resonance imaging (MRI) recordings from neuroscience. I was shortlisted for the second round, and after my talk, I was asked by a member of the committee, why I would not perform my own MRI experiment to collect my own data. I answered I did not want to do another MRI study with N = 30 patients. The person replied that there have been studies published in the journal *Nature* with only 7 mice. I then had to laugh because I thought it was a joke, but nobody else was laughing.

Well, here we are, years later, making deeper thoughts about this matter. In the present study, the sample size was not 7 mice. It was 12 mice, to be fair.

![Andy Warhol, Myths: Mickey Mouse from 1981 (Halcyon Gallery)](awa-spr-por-25979-4-f.jpg){width=50%}

In 2011, two comments about preclinical cancer research raised eyebrows [@Prinz2011-hm; @Begley2012-zy]. Apparently, several of landmark studies in high-impact journals that generated hundreds of secondary publications could not be verified by the pharmaceutical companies Bayer and Amgen. In these studies, investigators sometimes reported a single finding that supported their hypothesis, but that was often cherry-picking. That was the starting point of Tim Errington's investigation of reproducibility in preclinical cancer research [@Errington2021-gj]. Reasons for the irreproducibility of preclinical animal studies may be low sample size and, thus, low statistical power, as well as over-standardization of experiments [@Voelkl2020-nt]. Having read all this, Iâ€™m not so surprised anymore when another lab cannot confirm an exciting study finding from an animal experiment in a prestigious journal.

If a result cannot be repeated, what is it worth?

Reproducible animal studies require transparent reporting. The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) were initially developed in 2010 to improve the reporting of animal research, and in 2020, version 2.0 was released [@Percie_du_Sert2020-le].

# An exception to the rule

There are some exceptions in which stopping a study early can be justified. However, that's a whole different ballgame. Such situations need to be predefined, and adequate statistical methods must be used in the analysis. In clinical trials there is the term "interim analysis" were an analysis is conducted before the end of the study and data collection. This may be very useful when a treatment is clearly beneficial or harmful compared to the control arm [@Cook2022-dc]. Then, the investigator may stop the study early. However, as I said, this is all very well planned and defined in a study protocol and statistical analysis plan *a priori*.

For example, the Cardiac Arrhythmia Suppression Trial (CAST) was a four-arm trial investigating three antiarrhythmic drugs versus placebo on the risk of sudden cardiac death [@Echt1991-mh]. Early in the CAST study, two arms (encainide and flecainide) were stopped because of excess mortality compared to the placebo arm.

In the present mouse study, however, the authors stopped the experiment because the p-value fell below the magical $p \le 0.05$, apparently signaling to them that they could publish their results and didn't need to kill any more mice. If you perform enough tests, you will always obtain a p-value that is smaller than the magical threshold.

> "I'm looking for an interruption. Do you believe?" ---I don't sleep, I dream (song by R.E.M.)

I don't believe in stopping this study early, and I cannot trust the results. However, I believe that the study authors did not want to cheat. Most likely, the authors didn't know better. They thought the approach was okay, so they wrote it in the methods section. No statistician was on the author list, I guess, and no statistician reviewed this paper.

One last thought. When the authors of the study stopped the experiment, it was good for the next mouse whose life was saved. Unless the study was flawed.

Then all of the mice would be wasted.

*The title of this post is a song by R.E.M.; the preview image of this post is a photo by Karsten Winegeart on Unsplash.*

# Just one more thing

```{r}
#| code-fold: true

set.seed(2024)
N = 30 # number of mice
stop = 10:N # stop rules
k = 10000 # number of studies

ps = array(NA, dim = c(k, length(stop)))

# iterate across studies s
for (s in 1:k) {
  
  y1 = rnorm(N)
  y2 = rnorm(N)
  
  c = 1
  for (i in stop) {
    
    stat = wilcox.test(y1[1:i], y2[1:i], paired = TRUE)
    ps[s, c] = stat$p.value
    c = c + 1
    
  }
}
```

Let's perform a mouse study. We assume we planned $N = `r N`$ animals. However, we peep at the data already after `r stop[1]` were tested and then repeated testing after each additional animal until $p \leq 0.05$ or the total sample size was reached. As no animals were killed in my experiment here, we will perform $k = `r sprintf("%d", k)`$ of such studies.

In the mouse study, two measurements were obtained for each mouse. I will sample both observations from a normal distribution with a mean of 0 and standard deviation of 1, so the model behind the simulation assumes there is no difference in the pre and post-testing of the mice. Thus, every result I find is a fluke.

I performed the same test reported in the paper, a Wilcoxon signed-rank test, but that doesn't really matter to make the case. You could make the case with any statistical significance test that successfully controls the type I error at 5% to falsely reject the null.

Altogether, I collected $`r length(stop)` \times `r k`$ p-values (`r length(stop)` repeated inspections and `r sprintf("%d", k)` studies).

```{r}
#| code-fold: true

FP = sum(ps[,ncol(ps)] <= 0.05)
```
I looked at the last p-value for each study, i.e., after data from the total of $N = `r N`$ animals were collected. I found that `r FP` from `r sprintf("%d", k)` studies were statistically significant, which is a type I error rate of `r sprintf("%.3f", FP/k)`. Thanks God we are in control of the error. So far, so good.

```{r}
#| code-fold: true

# function peep() to get p value using repeated inspection
peep <- function(x) {
  
  d = data.frame(t = NA, p = NA)
  
  # pick first p value that is below cutoff
  if (min(x) <= 0.05) {
    i = which(x <= 0.05)[1] # pick first p value that is below cutoff
    d$t = i
    d$p = x[i]
    
    # if no success pick last p value
  } else {
    d$t = length(x)
    d$p = x[length(x)]
  }
  
  return(d)
}

myp = apply(ps, 1, peep)
myp = as.data.frame(t(array(unlist(myp), dim = c(2, k))))
colnames(myp) = c("t", "p")
```

Now, the twist. I peeped at the other p-values. I picked the p-value as soon as it turned statistically significant and stopped the experiment. But when I was not lucky enough to observe a $p \leq 0.05$, I collected the p-value from the last repeated test with $N = `r N`$ animals, whatever that value looked like.

```{r}
#| code-fold: true

FP = sum(myp$p <= 0.05)
```

I found that `r FP` from `r sprintf("%d", k)` studies were statistically significant, which is a type I error rate of `r sprintf("%.2f", FP/k)`. Now, we lost control. The type I error of falsely rejecting the null has tripled.

```{r}
#| fig-height: 2.5
#| fig-width: 5
#| code-fold: true
#| fig-cap: "Figure 1: Two example studies that stopped early due to peeping at the p-value before the study was completed. The red data point highlights the statisticially significant p-value where the study was stopped early. In (A), the study stopped after observing 10 mice, and (B) after observing 25 mice. The horizontal line shows the significance threshold at 0.05. As we can see, the p-value is very wiggly and can be exploited by bad research practices such as the repeated inspection of data."

library(ggplot2)
library(gridExtra)

N_plots = 2

# pick two significant studies
set.seed(2024)
idx = which(myp$p <= 0.05)
idx = sample(idx, N_plots)

p = list()

for (i in 1:N_plots) {
  
  d = data.frame(p = ps[idx[i],], stop = stop)
  
  p[[i]] = ggplot(d, aes(x = stop, y = p, col = "red")) + 
    geom_line() + 
    geom_point(size = 2) +
    geom_point(data = myp[idx[i],], 
               size = 2,
               mapping = aes(x = t + stop[1] - 1,
                             y = p, col="green")) +
    ylim(c(0, 1)) +
    geom_hline(yintercept = 0.05) +
    scale_color_manual(values = c("#BF505A", "#6F87A6")) +
    labs(title = paste("Study", i), tag = LETTERS[i]) +
    xlab("Number of mice observed") + ylab("p-value") +
    theme_minimal() +
    theme(legend.position = "none")
}

do.call("grid.arrange", c(p, nrow = 1, ncol=2))
```

I highlight two case studies from the `r FP` positive findings that were statistically significant by peeping. Study 1 was statistically significant at the first glimpse, so we can already go home after 10 mice. Brilliant. Study 2 was statistically significant after 25 mice. Notice that if we observed only the last p-value at the end, after completing the `r N` mice, then both study 1 and study 2 failed to be statistically significant. This finding would be correct since I was the creator of the data and knew the truth behind the data.

The truth was, there was nothing there.

## So what?

First, a p-value from repeated inspection of data is nothing more than a fluke of the universe. Don't trust them. Second,  *Nature Communications* may consider involving expert statisticians in the assessment of a study. After all, their article processing charges amount to 6790 US dollars.
