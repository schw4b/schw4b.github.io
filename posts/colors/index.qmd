---
title: "True colors"
draft: false
description: "Science is often viewed as a reliable mechanism for producing facts. However, a large proportion of scientific findings may be incorrect. Even when a low p-value is obtained, it does not guarantee that the result is true; in fact, it may still be wrong. This claim can be effectively illustrated using an example involving colors which reveal how many studies were true and how many false."
author: "Simon Schwab"
date: "2024-11-24"
categories: [p-values]
image: "drew-beamer-Hk6E4UxjmGo-unsplash.jpg"
citation:
  issued: 2024
  type: post-weblog
  url: https://www.statsyup.org/posts/colors/
bibliography: references.bib
google-scholar: true
csl: plos.csl 
---

Science has a high credibility and reputation. The public, the media, and politics would usually believe that a scientific result is true. After all, it is science, isn't it? If we don't believe science, what should we believe in?

However, scientific findings are often based on weak statistical results. The quality of statistics is often lacking in the published scientific literature. I may be delving into a controversial topic here, and I will only focus on a single aspect: the widespread misunderstanding of the p-value.

> "The problem is not that people use p-values poorly. It is that the vast majority of data analysis is not performed by people properly trained to perform data analysis”. ---Jeff Leek

I would say it differently. The vast majority of data analysis is performed by people not properly trained. Therefore, p-values are indeed used poorly as a consequence.

The poor understanding and use of p-values made the American Statistical Association (ASA) release a statement in 2016 [@Wasserstein2016-nu]. The ASA argued that the p-values are commonly misused and misinterpreted. Many researcher believe a low p-value is some form of insurance that the result is credible. For example, a p-value of 0.012 may indicate that there is some high probability that the results is *real*, and therefore,  the result is meaningful and worth publishing. I can show in an example below that this is not the case.

A scientific results is worth publishing when the study was well planned, designed, conducted, and reported. Only study protocols with a statistical analysis plan can assure this. The p-value on itself does not matter at all. In a perfect world, we would see mostly high-quality studies with both low and high p-values. The opposite is true, there are too many low-quality studies, and too many studies report exclusively low p-values.

# What does the p-value actually mean?

The way I interpret the p-value is that it is a probability how compatible the data are with a specified model. This model is under the null hypothesis which often says that there is no difference between two groups, or there is an absence of a relationship between a variable and an outcome. The smaller the p-value, the greater this incompatibility of the data with the null hypothesis. Thus, a small p-value may then provide evidence against the null hypothesis.

Here is how the ASA explained the p-value:

> Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. ---ASA Statment

In other words, it is a probability statement about the data, and not about the truth of a hypothesis. The most common misunderstanding is that a p-value of 0.012 reflects a 1.2% chance of the result being a false alarm. This interpretation is not correct. Most researchers may not be aware that the commonly used alpha level of 5% is absolutely no insurance that the result is not a fluke. In fact, p-values, the alleged gold standard of statistical validity are not as reliable as many researchers believe [@Nuzzo2014-wf].

# A lot of science may be wrong

When looking at the body of, for example, medical research, I wonder, how many results are correct? How many results are incorrect? What is the probability that a research finding is false? Unfortunately, the answer is not exactly amusing [@Altman1994-vw; @Ioannidis2005-pn; @Colquhoun2014-hb].

To get an approximate answer for a specific situation. I consider a 1,000 studies, each is well designed and conducted. I assume the widely used alpha level of 5%, meaning that in 5 of 100 studies where there was no effect, I will find a positive result; however, these results are false positive results, also known as the type I error. Furthermore, I assume all the studies have a statistical power of 80%. Thus, among the studies where there really was a true effect, I will find it in 80 of 100 studies. Consequently, in 20 out of 100 studies, however, I will miss the true result; these are the false negatives, also known as the type II error. My last assumption is about the question, how many hypothesis are really true? I assume a study area where many hypothesis turn out to be true, say, in 30% of the cases.

Imagine a 1,000 hypotheses under investigation, where 3 in 10 are true. Thus, 300 are true and 700 are false. From the 700 false hypotheses, we still find 5% to be falsely positive, which are 35. From the 300 true hypotheses, we find a positive result in 80% of the cases, which are 240. Altogether, we see 35 and 240 results as positive results; however, the 35 are false results, which is 13%.

Thus, more than 10% of the results are wrong, even though we used the common alpha level of 5%. However, this really is the best case scenario with a alpha level of 5% and a statistcial power of 80%,

In reality, however, the reported standard errors and p-values are not honest. The reason is that data was often reanalyzed until the desired result was obtained. Thus, many reported results have probably a much higher p-values if data was analyzed according to a prespecified protocol. In other words, the 5% alpha level is in reality maybe 10% or higher due to the bias in the statistical analysis. Furthermore, the statistical power of 80% is from a perfect world, and this assumption is not realistic at all even in the domain of highest standards such as clinical trials. I don't know that the true statistical power might be, maybe it is as low as 50%. Lastly, there are domains where true hypotheses are not so prevalent, for example, where only 1 in 10 hypotheses are in fact true.

Making a scientific claim in a domain where novel findings are hard to discover, where p-hacking is prevalent, and where sample size calculations and statistical power are dismissed makes a result very unlikely to be true, even if the p-value is statistically significant. We cannot ignore the process of how the p-value was obtained. We must have some prior skeptical believe when looking at each p-values.

When I look at a p-value I don't see them in terms of statistically significant or not. I look at a p-value an ask myself, can I believe a finding when taking other information into the equation, for example, the study design, the sample size, the statistical analysis, and how results have been reported? If I had no  prior skepticism and just look at the magnitiude of a p-values, ignoring everything else, I can equally likely believe in Santa Clause.

Or let my make a better example. Ignoring the story behind a p-value and take it as face value would be like waking up one morning with a headache, and conclude that you have a rare brain tumor.

Fortunately, however, it is an unlikely result.

*The title of this post is a song by Cindy Lauper. The preview image of this post is a photo by Drew Beamer on Unsplash.*

# Just one more thing

Below I used some plots to illustrate that p-values are not as reliable as many researchers believe. In these plots, each square was a hypothesis which was either true or false. Each hypothesis was tested by a study, and ased on some assumptions about the proportions of true hypotheses, the alpha level, and the statistical power, I determined how likely scientific results may be. This approach was strongly inpired by an article and video by the Economist [@Unknown2013-ji]. 

```{r}
#| eval: true
#| code-fold: true

library(ggplot2)
library(reshape2)

k = 1000

COLORS = c("#BF505A", "#D9A282", "#818C70", "#B4BF8A")
```

```{r}
#| eval: true
#| code-fold: true

# k: number of hypotheses
# p: prop. of true hypotheses
# alpha: alpha level
# power: statistical power

true_colors <- function(k, p_true, alpha, power) {
  
  t = p_true*k
  f = k - t
  M = matrix(1:k, 20)
  data = melt(M, varnames = c("row", "col"), value.name = "id")
  
  data$H = "false"
  data$H[1:t] = "true"
  
  # true and false hypotheses
  data$H = factor(data$H, levels = c("false", "fn", "true", "fp"))
  
  # add false pos and false neg
  data$H_ = data$H
  
  fp = round(f*alpha)
  idx = sample(which(data$H_ == "false"), fp)
  data$H_[idx] = "fp"
  
  fn = round(t*(1 - power))
  idx = sample(which(data$H_ == "true"), fn)
  data$H_[idx] = "fn"
  
  pos = sum(data$H_ == "true")
  neg = sum(data$H_ == "false")
  fp  = sum(data$H_ == "fp")
  fn  = sum(data$H_ == "fn")
  fdr = fp / (pos + fp)
  
  return(list(data = data, k = k, p_true = p_true, alpha = alpha, power = power,
              pos = pos, neg = neg, fp = fp, fn = fn, fdr = fdr))
  
}

set.seed(2024)
mylist = true_colors(k = k, p_true = 0.30, alpha = 0.05, power = 0.80)
```

I considered `r mylist$k` hypotheses. I assumed that `r sprintf("%d%%",  mylist$p_true*100)` of the hypotheses were true, see Figure 1.

```{r}
#| fig-height: 2
#| fig-width: 5
#| code-fold: true
#| fig-cap: "Figure 1: Visualization of 1,000 hypotheses, of which 300 were true (green) and 700 were false (red)."

ggplot(mylist$data, aes(x=col, y=row, fill=H)) +
  geom_tile(color = "gray90") +
  coord_equal() +
  theme_void() +
  scale_fill_manual(values = COLORS[c(1, 3)], guide = guide_none())
```

Imagine each hypothesis was tested by a study. Given an alpha level of `r mylist$alpha`, there were `r mylist$fp` studies that produced a false positive result (type I error). The studies also had a power of `r mylist$power`; thus, `r mylist$fn` of the studies were false negatives (type II error), see Figure 2.

```{r}
#| fig-height: 2
#| fig-width: 5
#| code-fold: true
#| fig-cap: "Figure 2: Visualization of studies that tested 1,000 hypotheses. Some studies produced a false positive result among the false hypotheses (light green). Some studies produced a false negative result among the true hypotheses (light red)."

ggplot(mylist$data, aes(x=col, y=row, fill=H_)) +
  geom_tile(color = "grey90") +
  coord_equal() +
  theme_void() +
  scale_fill_manual(values = COLORS, guide = guide_none())
```

In reality we only see the positive results which were the `r mylist$pos` positive results (green) and the `r mylist$fp` false positive results (light green). Therefore, `r mylist$fp` from the total of `r sum(mylist$pos, mylist$fp)` positive results were wrong. This proportion is `r sprintf("%.2f", mylist$fdr)` and is also known as the false discovery rate (FDR). Note that this error rate is more than double as high as the convenient alpha level of 0.05.



```{r}
#| eval: true
#| code-fold: true

set.seed(2024)
mylist = true_colors(k = k, p_true = 0.10, alpha = 0.10, power = 0.50)
```

```{r}
#| fig-height: 2
#| fig-width: 5
#| code-fold: true
#| fig-cap: "Figure 3: Visualization of studies that tested 1,000 hypotheses. Some studies produced a false positive result among the false hypotheses (light green). Some studies produced a false negative result among the true hypotheses (light red)."

ggplot(mylist$data, aes(x=col, y=row, fill=H_)) +
  geom_tile(color = "grey90") +
  coord_equal() +
  theme_void() +
  scale_fill_manual(values = COLORS, guide = guide_none())
```
